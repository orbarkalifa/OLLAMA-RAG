{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5434f62b",
   "metadata": {},
   "source": [
    "# 🧪 Exercise 3: PDF Question Answering using Chunking, Vector Search, and LLM\n",
    "\n",
    "In this exercise, you'll complete a **retrieval-augmented generation (RAG)** pipeline that:\n",
    "- Chunks and embeds the content of a PDF\n",
    "- Stores the chunks in an in-memory vector database (Qdrant)\n",
    "- Uses a local LLM to answer yes/no/unknown questions based strictly on the PDF content\n",
    "\n",
    "You will implement the missing components of this pipeline, focusing on document chunking, retrieval, and prompt construction.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Goal\n",
    "\n",
    "Your objective is to build a system that can answer **yes**, **no**, or **unknown** questions based solely on the information in a given **PDF file**.\n",
    "\n",
    "- The answer must be **one word only**: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`\n",
    "- The LLM must not use external knowledge — it must rely **only** on content retrieved from the PDF\n",
    "- The total prompt sent to the LLM is limited to **2000 characters**, including:\n",
    "  - The instruction\n",
    "  - Retrieved context chunks  \n",
    "  *(🚫 The question itself is not included in this limit and will be added separately)*\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What You Need to Do\n",
    "\n",
    "You must implement the following three core functions:\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ `chunk_and_store(text: str) -> tuple[QdrantClient, SentenceTransformer]`\n",
    "\n",
    "This function prepares the document for retrieval.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Chunk the input `text` into smaller segments\n",
    "- Encode each chunk into a vector using a pre-trained embedding model such as `all-MiniLM-L6-v2`\n",
    "- Store the vectors in an in-memory **Qdrant** database using `QdrantClient(\":memory:\")`\n",
    "- Store relevant metadata for each chunk (e.g., start offset, method)\n",
    "\n",
    "**Returns:**\n",
    "- `client`: a Qdrant client object that contains the embedded chunks\n",
    "- `model`: the SentenceTransformer model used for encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ `create_prompt(question: str, client: QdrantClient, model: SentenceTransformer) -> str`\n",
    "\n",
    "This function builds the prompt to be sent to the LLM.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Retrieve the top-k most relevant chunks from Qdrant using the question as a query\n",
    "- Construct a prompt that includes:\n",
    "  - A fixed instruction (you may define this in the function)\n",
    "  - The most relevant retrieved chunks\n",
    "- The full prompt must not exceed **400 characters**, excluding the question\n",
    "\n",
    "**Returns:**\n",
    "- A string containing the prompt (instruction + context), **excluding** the question\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ✅ `my_call_llm(prompt: str, question: str) -> str`\n",
    "\n",
    "This function provides an interface to the LLM, but must not invoke the LLM directly.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Optionally apply logic to enhance or adapt the query (e.g., pre-processing the prompt, logging, enforcing formatting rules)\n",
    "- Call the provided `call_llm(prompt, question)` function to actually interact with the model\n",
    "- Return the result unchanged, or with controlled, explainable adjustments **that do not modify the content of the LLM’s response**\n",
    "\n",
    "**Rules:**\n",
    "- ❌ Must **not** embed, re-embed, or analyze any part of the original document or its chunks\n",
    "- ❌ Must **not** call `subprocess`, `ollama`, or any direct LLM API\n",
    "- ✅ Must **only** call `call_llm(prompt, question)` to obtain the response\n",
    "\n",
    "**Returns:**\n",
    "- A string (typically `\"Yes\"`, `\"No\"`, or `\"Unknown\"`) returned by the LLM, possibly post-processed for stability, format, or logging\n",
    "\n",
    "**Purpose:**\n",
    "- This function acts as a controlled gateway to LLM usage, allowing improvements in how prompts are used or tracked, without modifying or reprocessing the document or query logic\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> 💡 **Tip:** You are encouraged to define helper functions to simplify your code and improve readability.\n",
    "\n",
    "---\n",
    "\n",
    "### ✨ Provided Function (DO NOT CHANGE)\n",
    "\n",
    "#### ✅ `call_llm(prompt: str) -> str`\n",
    "\n",
    "This function is already implemented for you.\n",
    "\n",
    "- It calls the `llama3.2:3b` model via `ollama`\n",
    "- Receives the question and your constructed prompt\n",
    "- Returns the LLM's answer (expected: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`)\n",
    "\n",
    "You do **not** need to re-implement or modify this function.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Evaluation Criteria\n",
    "\n",
    "- Your system will be evaluated using a **corpus of 100 questions** on a **known PDF document**\n",
    "- You will be given in advance a **sample of 20 questions** from the evaluation corpus for development and testing\n",
    "- Your code must generate the correct **yes/no/unknown** answers for the full 100-question set\n",
    "- **Total execution time** will be measured for the entire run (reading, chunking, querying, and answering)\n",
    "\n",
    "---\n",
    "\n",
    "### 🚫 Restrictions\n",
    "\n",
    "- **Do not** modify any code cell marked with `# DO NOT CHANGE`.\n",
    "- **Do not** override any variable or function defined in those protected cells.\n",
    "- Your code must run successfully in the **Lab 10002** environment (`GenAI025_CUDA`), using only the libraries provided.\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 **Tip:** Write clean, modular code. Aim for accuracy, clarity, and runtime efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Good luck!\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Points Deduction Rules\n",
    "\n",
    "1. **Modifying restricted code**  \n",
    "   - Changing any `# DO NOT CHANGE` cell or variable: **–50 points**\n",
    "\n",
    "2. **Importing any additional library**  \n",
    "   - Importing any library that is **not already used** in the template: **–5 points per library**  \n",
    "   - ✅ *No penalty* for importing additional modules or functions from libraries that are already used (e.g., importing more from `langchain` or `sentence_transformers`)\n",
    "\n",
    "3. **Code compatibility**  \n",
    "   - Code fails to run in Lab 10002: **–100 points**\n",
    "\n",
    "4. **Execution time (total run of 100 questions)**  \n",
    "   - Runs for **5–10 minutes**: **–30 points**  \n",
    "   - Runs for **>10 minutes**: **–100 points**\n",
    "\n",
    "5.  **Violating restrictions inside `my_call_llm()`**  \n",
    "   - ❌ Must **not** embed, re-embed, or analyze any part of the original document or its chunks  \n",
    "   - ❌ Must **not** call `subprocess`, `ollama`, or any direct LLM API  \n",
    "   - ✅ Must **only** call `call_llm(prompt, question)` to obtain the response  \n",
    "   - Penalty: **–100 points**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Final Score Calculation\n",
    "\n",
    "$$\n",
    "\\text{Final Score} = \\min \\left(100,\\ \\frac{\\text{Your correct answers}}{\\text{Gadi’s correct answers}} \\times 100 \\right) - \\text{Total Deductions}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "📌 *Submit clean, working code. Only modify what you're allowed to. You got this!*\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d495b5d6-7594-41b4-a568-1fe3e971acef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:07.944226Z",
     "start_time": "2025-05-12T10:59:07.941548Z"
    }
   },
   "source": [
    "# SET PATH According to your configuration.\n",
    "PDF_PATH = \"MyBank Credit Card Brochure.pdf\"\n",
    "QUESTIONS_PATH = \"questions.txt\"\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "afada56a-bf9e-4f76-a4fa-ad69f8e43b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:08.138655Z",
     "start_time": "2025-05-12T10:59:07.963445Z"
    }
   },
   "source": [
    "# DO NOT CHANGE\n",
    "import fitz\n",
    "import os\n",
    "import uuid\n",
    "import spacy\n",
    "import subprocess\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    NLTKTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, PointStruct\n",
    "import time"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# DO NOT CHANGE\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfitz\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01muuid\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'fitz'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c8d21727",
   "metadata": {},
   "source": [
    "# DO NOT CHANGE\n",
    "def load_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "    return text\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25e838a6",
   "metadata": {},
   "source": [
    "# DO NOT CHANGE\n",
    "def load_questions(questions_path):\n",
    "    with open(questions_path, \"r\") as f:\n",
    "        questions = [q.strip() for q in f.readlines() if q.strip().endswith('?')]\n",
    "    return questions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TOKENIZERS_PARALLELISM = \"false\"  # Disable parallelism for tokenizers to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = TOKENIZERS_PARALLELISM\n",
    "# HELPER FUNCTIONS AND GLOBAL VARIABLES\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Model and Collection ---\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "# Define a FIXED collection name that both chunk_and_store and create_prompt will use\n",
    "COLLECTION_NAME = \"rag_exercise_pdf_collection_fixed_v1\" # Using a specific, fixed name\n",
    "\n",
    "# --- Chunking Parameters ---\n",
    "CHUNK_METHOD = \"RecursiveCharacterTextSplitter\"\n",
    "CHUNK_SIZE = 128\n",
    "CHUNK_OVERLAP = 40\n",
    "\n",
    "# --- Retrieval and Prompting Parameters ---\n",
    "TOP_K = 5                            # Number of top relevant chunks to retrieve - TUNE THIS\n",
    "PROMPT_CONTEXT_LIMIT = 2000           # Max characters for the final prompt (instruction + context)\n",
    "\n",
    "# --- Helper Function: find_chunks ---\n",
    "def find_chunks(\n",
    "    question: str,\n",
    "    client: QdrantClient,\n",
    "    model: SentenceTransformer,\n",
    "    collection_name: str,\n",
    "    top_k: int = TOP_K\n",
    "):\n",
    "    \"\"\"\n",
    "    Encodes the question and retrieves the top_k most relevant chunks\n",
    "    from Qdrant, using the new `query_points` API to avoid deprecation.\n",
    "    \"\"\"\n",
    "    if not collection_name:\n",
    "        print(\"Error: Collection name is not set for find_chunks.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"  Searching for top {top_k} chunks in collection '{collection_name}'...\")\n",
    "    try:\n",
    "        # 1. Embed the question\n",
    "        question_vector = model.encode(question).tolist()\n",
    "\n",
    "        # 2. Use the new query_points method (search is deprecated)\n",
    "        response = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=question_vector,\n",
    "            limit=top_k,\n",
    "        )\n",
    "        # `response.points` is a list[ScoredPoint]\n",
    "        search_results = response.points\n",
    "\n",
    "        print(f\"  Found {len(search_results)} candidate chunks.\")\n",
    "        return search_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during Qdrant search in find_chunks: {e}\")\n",
    "        return []\n"
   ],
   "id": "ae127caedb954f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8f47aeb",
   "metadata": {},
   "source": [
    "### ✅ Task 1: Implement `chunk_and_store(text)`"
   ]
  },
  {
   "cell_type": "code",
   "id": "46b8a3c6",
   "metadata": {},
   "source": [
    "def chunk_and_store(text: str):\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller chunks and stores them in a vector database or an internal memory structure.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed. This should be a large block of text (e.g., a document, an article, or a report).\n",
    "\n",
    "    Behavior:\n",
    "    --------\n",
    "    1. The function splits the input `text` into manageable chunks based on predefined chunking rules \n",
    "       (e.g., maximum character count, sentence boundaries, semantic meaning).\n",
    "    2. Each chunk is optionally enriched with metadata (e.g., chunk number, character offsets, original document ID).\n",
    "    3. Each chunk is stored in a storage system such as:\n",
    "       - An in-memory list or dictionary (for simple setups)\n",
    "       - A vector database (e.g., Qdrant, FAISS, ChromaDB) after embedding the chunk using an encoder model\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    client : qdrant_client.QdrantClient\n",
    "        A Qdrant client object that contains the embedded and stored chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used for embedding the text chunks.\n",
    "   \n",
    "    Notes:\n",
    "    -----\n",
    "    - If using a vector database, the chunk is first passed through an embedding model to create a vector representation.\n",
    "    - Chunking methods might vary (e.g., fixed-size, sentence-based, semantic-split) depending on implementation details.\n",
    "    - The function assumes that the storage backend is already initialized and ready for storing chunks.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `text` is empty or not a valid string.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> chunk_and_store(\"This is a long article about machine learning...\")\n",
    "    # Splits the article into chunks and stores them internally or externally.\n",
    "\n",
    "    \"\"\"\n",
    "    # Implementation goes here\n",
    "    # TODO: implement chunking using multiple strategies\n",
    "    # TODO: create in-memory Qdrant collection\n",
    "    # TODO: embed each chunk and store in the DB with metadata (chunking method, start_offset)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Splits text into chunks using RecursiveCharacterTextSplitter, embeds them,\n",
    "    and stores them in an in-memory Qdrant vector database using the globally\n",
    "    defined COLLECTION_NAME.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed (e.g., content of a PDF).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    client : qdrant_client.QdrantClient\n",
    "        A Qdrant client object connected to the in-memory database containing the embedded chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used for embedding the text chunks.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `text` is empty or not a valid string.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(\"Input text cannot be empty and must be a string.\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Starting chunking and storing process...\")\n",
    "    print(f\"Input text length: {len(text)} characters\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Initialize Embedding Model\n",
    "    print(f\"Loading sentence transformer model: {MODEL_NAME}\")\n",
    "    try:\n",
    "        model = SentenceTransformer(MODEL_NAME)\n",
    "        embedding_size = model.get_sentence_embedding_dimension()\n",
    "        if embedding_size is None:\n",
    "             raise ValueError(\"Could not determine embedding dimension from the model.\")\n",
    "        print(f\"Model loaded successfully. Embedding dimension: {embedding_size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer model '{MODEL_NAME}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # 2. Initialize Qdrant Client (In-Memory)\n",
    "    print(\"Initializing in-memory Qdrant client...\")\n",
    "    try:\n",
    "        client = QdrantClient(\":memory:\")\n",
    "        print(\"Qdrant client initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Qdrant client: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 3. Create Qdrant Collection using the global COLLECTION_NAME\n",
    "    print(f\"Creating or recreating Qdrant collection: '{COLLECTION_NAME}'\")\n",
    "    try:\n",
    "        client.recreate_collection(\n",
    "            collection_name=COLLECTION_NAME, # Use the global constant\n",
    "            vectors_config=models.VectorParams(size=embedding_size, distance=models.Distance.COSINE)\n",
    "        )\n",
    "        print(f\"Collection '{COLLECTION_NAME}' created/recreated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/recreating Qdrant collection '{COLLECTION_NAME}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # 4. Chunk the Text using the chosen strategy (defined globally)\n",
    "    print(f\"Chunking text using {CHUNK_METHOD} (Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP})\")\n",
    "    try:\n",
    "        if CHUNK_METHOD == \"RecursiveCharacterTextSplitter\":\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                length_function=len,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "        else:\n",
    "            # Add other methods here if needed, referencing CHUNK_METHOD\n",
    "            raise ValueError(f\"Unsupported chunking method specified globally: {CHUNK_METHOD}\")\n",
    "\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        print(f\"Text split into {len(chunks)} chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text chunking: {e}\")\n",
    "        raise\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"Warning: No chunks were generated from the text. Returning empty client.\")\n",
    "        return client, model\n",
    "\n",
    "    # 5. Embed Chunks and Prepare Points for Qdrant\n",
    "    print(\"Embedding chunks and preparing points for Qdrant...\")\n",
    "    points_to_upsert = []\n",
    "    chunk_processing_start_time = time.time()\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        if not chunk_text.strip():\n",
    "            print(f\"Skipping empty chunk at index {i}\")\n",
    "            continue\n",
    "        try:\n",
    "            vector = model.encode(chunk_text).tolist()\n",
    "            payload = { \"text\": chunk_text, \"chunk_index\": i, \"method\": CHUNK_METHOD }\n",
    "            # Use a deterministic or random UUID for point IDs\n",
    "            point_id = str(uuid.uuid4()) # Use random UUID\n",
    "            # Or use deterministic ID if needed:\n",
    "            # point_id = uuid.uuid5(uuid.NAMESPACE_DNS, f'{COLLECTION_NAME}_{i}_{chunk_text[:20]}').hex\n",
    "\n",
    "            point = PointStruct(id=point_id, vector=vector, payload=payload)\n",
    "            points_to_upsert.append(point)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing or embedding chunk {i}: {e}\")\n",
    "            raise\n",
    "\n",
    "    chunk_processing_end_time = time.time()\n",
    "    print(f\"Embedding and point preparation took {chunk_processing_end_time - chunk_processing_start_time:.2f} seconds.\")\n",
    "\n",
    "    # 6. Upsert Points to Qdrant Collection using the global COLLECTION_NAME\n",
    "    if points_to_upsert:\n",
    "        print(f\"Upserting {len(points_to_upsert)} points into collection '{COLLECTION_NAME}'...\")\n",
    "        try:\n",
    "            client.upsert(\n",
    "                collection_name=COLLECTION_NAME, # Use the global constant\n",
    "                points=points_to_upsert,\n",
    "                wait=True\n",
    "            )\n",
    "            print(\"Upsert operation successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting points into Qdrant: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(\"No valid points were generated to upsert.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Chunking and storing completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 7. Return only the client and model, as per original signature\n",
    "    return client, model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4169634c",
   "metadata": {},
   "source": [
    "### ✅ Task 2: Implement `create_prompt(question, client, model)`"
   ]
  },
  {
   "cell_type": "code",
   "id": "e771f685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:08.152456Z",
     "start_time": "2025-05-12T10:59:08.146809Z"
    }
   },
   "source": [
    "def create_prompt(question: str, client, model):\n",
    "    \"\"\"\n",
    "    Creates a context-only prompt for an LLM by retrieving relevant chunks from a vector database \n",
    "    based on a user question, using a vector similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    question : str\n",
    "        The input question provided by the user. It should be a natural language query.\n",
    "\n",
    "    client : qdrant_client.QdrantClient\n",
    "        The Qdrant client connected to the database that contains stored and embedded text chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used to encode the input question into a vector embedding \n",
    "        for similarity search.\n",
    "\n",
    "    Behavior:\n",
    "    --------\n",
    "    1. The function encodes the input `question` into a vector using the provided `model`.\n",
    "    2. It queries the `client` (Qdrant database) using vector similarity search to find the most relevant chunks.\n",
    "    3. It assembles a prompt by combining the retrieved chunks and other info (but without adding the question itself).\n",
    "    4. The resulting prompt consists **only of context**, intended to be passed separately along with the question \n",
    "       in a later step when calling the LLM.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    prompt : str\n",
    "        A fully formatted prompt string. \n",
    "        **The user's question is NOT included in the returned prompt.**\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The search typically retrieves the top-k most similar chunks (e.g., top 5).\n",
    "    - Retrieved chunks are usually concatenated together, separated by delimiters (e.g., \"\\n\\n\").\n",
    "    - The question should be provided separately to the LLM after sending the prompt, or combined externally later.\n",
    "    - This function assumes that both the client and model are already initialized and ready to use.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `question` is empty or not a valid string.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> context_prompt = create_prompt(\"What benefits does the Platinum Voyager Card offer?\", client, model)\n",
    "    >>> print(context_prompt)\n",
    "    \"Context:\\n<retrieved chunks>\"\n",
    "\n",
    "    # Later, when calling the LLM:\n",
    "    # final_prompt = context_prompt + \"\\n\\nQuestion:\\nWhat benefits does the Platinum Voyager Card offer?\"\n",
    "    \"\"\"\n",
    "    # TODO: use find_chunks()\n",
    "    # TODO: build the prompt with CONTEXT_HEADER and top chunks\n",
    "    # TODO: truncate to PROMPT_CHAR_LIMIT if needed\n",
    "\n",
    "    \"\"\"\n",
    "    Builds an instruction + context prompt (≤400 chars), then\n",
    "    DEBUG-prints how many chunks were used and the actual prompt.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Creating prompt for question: '{question[:100]}...'\")\n",
    "\n",
    "    if not question or not isinstance(question, str):\n",
    "        raise ValueError(\"Input question must be a non-empty string.\")\n",
    "\n",
    "    # 1) Instruction + delimiter\n",
    "    instruction = (\n",
    "        \"Answer using ONLY the Context below. Respond with 'Yes', 'No', or 'Unknown'. \"\n",
    "        \"If context is insufficient, answer 'Unknown'.\\n\\n\"\n",
    "        \"Context:\\n---\\n\"\n",
    "    )\n",
    "    end_delim = \"\\n---\"\n",
    "\n",
    "    # 2) Retrieve top-k chunks\n",
    "    print(f\"Finding relevant chunks in '{COLLECTION_NAME}' (TOP_K={TOP_K})...\")\n",
    "    relevant_chunks = find_chunks(\n",
    "        question=question,\n",
    "        client=client,\n",
    "        model=model,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    print(f\"  Retrieved {len(relevant_chunks)} candidate chunks.\")\n",
    "\n",
    "    # 3) Assemble context under the 400-char cap\n",
    "    print(f\"Building context string (limit {PROMPT_CONTEXT_LIMIT} chars)...\")\n",
    "    context_parts = []\n",
    "    current_length = len(instruction) + len(end_delim)\n",
    "    added_chunks_count = 0\n",
    "\n",
    "    for i, hit in enumerate(relevant_chunks):\n",
    "        chunk_text = hit.payload.get(\"text\", \"\").replace(\"\\n\", \" \").strip()\n",
    "        if not chunk_text:\n",
    "            continue\n",
    "\n",
    "        # account for a newline between chunks\n",
    "        length_needed = len(chunk_text) + (1 if context_parts else 0)\n",
    "\n",
    "        if current_length + length_needed <= PROMPT_CONTEXT_LIMIT:\n",
    "            context_parts.append(chunk_text)\n",
    "            added_chunks_count += 1\n",
    "            current_length += length_needed\n",
    "            print(f\"  Added chunk {i} (len={len(chunk_text)}). Total so far: {current_length}/{PROMPT_CONTEXT_LIMIT}\")\n",
    "        else:\n",
    "            print(f\"  Skipped chunk {i} (would exceed: {current_length + length_needed}/{PROMPT_CONTEXT_LIMIT})\")\n",
    "            break\n",
    "\n",
    "    # 4) Final prompt\n",
    "    context_string = \"\\n\".join(context_parts)\n",
    "    final_prompt = instruction + context_string + end_delim\n",
    "\n",
    "    # ── DEBUG OUTPUT ──\n",
    "    print(f\"\\nDEBUG: Chunks added to prompt: {added_chunks_count}\")\n",
    "    print(f\"DEBUG: Final prompt length: {len(final_prompt)} chars\")\n",
    "    print(\"DEBUG: Prompt content:\\n\" + final_prompt)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return final_prompt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8b50937f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:08.430740Z",
     "start_time": "2025-05-12T10:59:08.427511Z"
    }
   },
   "source": [
    "# DO NOT CHANGE\n",
    "# LLM via Ollama\n",
    "def call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls a local LLM using the Ollama CLI and returns the model's response.\n",
    "\n",
    "    This function sends a prompt to the locally hosted `llama3.2:3b` model via the `ollama` command-line interface.\n",
    "    It ensures the prompt does not exceed 500 characters and captures the model's output.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The full input prompt to be sent to the LLM. It should include context and instructions,\n",
    "                      but not the question itself if using external control.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw response generated by the model. If the model call times out, returns \"Unknown\".\n",
    "\n",
    "    Notes:\n",
    "        - The prompt is truncated to a maximum of 2000 characters before being sent.\n",
    "        - The model is expected to return a one-word answer such as \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    prompt = prompt[:2000] + \"\\nQuestion: \" + question\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3.2:3b\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=30\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Unknown\"\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "cd328aed-0af0-4800-bdb4-89c00011f208",
   "metadata": {},
   "source": [
    "### ✅ Task 3: Implement `my_call_llm(prompt: str, question: str)`"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b62c34f-e34c-49bf-8036-673318ac2742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:08.481799Z",
     "start_time": "2025-05-12T10:59:08.476635Z"
    }
   },
   "source": [
    "def my_call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    A wrapper function for controlled interaction with the local LLM.\n",
    "\n",
    "    This function allows for preprocessing, logging, or evaluation logic\n",
    "    around a call to the provided `call_llm()` function, but it must not\n",
    "    directly interact with the LLM (e.g., via subprocess or embedding logic).\n",
    "\n",
    "    🚫 Restrictions:\n",
    "        - Must NOT embed, re-embed, or analyze any part of the original document or its chunks\n",
    "        - Must NOT call `subprocess`, `ollama`, or any direct LLM APIs\n",
    "        - Must ONLY interact with the LLM via the provided `call_llm(prompt, question)` function\n",
    "\n",
    "    ✅ Allowed:\n",
    "        - Logging or printing\n",
    "        - Handling empty prompts or question formats\n",
    "        - Calling `call_llm()` multiple times for retry logic or consistency checking\n",
    "        - Standard string manipulations (if needed)\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The constructed prompt (instructions + context, excluding the question).\n",
    "        question (str): The original user question (to be passed to the LLM interface).\n",
    "\n",
    "    Returns:\n",
    "        str: - return a one-word answer typically one of \"Yes\", \"No\", or \"Unknown\".\n",
    "\n",
    "    Example:\n",
    "        >>> my_call_llm(\"Context: Data is collected by Google...\", \"Does Google share my location?\")\n",
    "        \"Yes\"\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    A wrapper function for controlled interaction with the local LLM via the\n",
    "    provided call_llm function, ensuring the final output is one of \"Yes\", \"No\", or \"Unknown\".\n",
    "\n",
    "    🚫 Restrictions:\n",
    "        - Must NOT embed, re-embed, or analyze document content or chunks.\n",
    "        - Must NOT call subprocess, ollama, or any direct LLM API.\n",
    "        - Must ONLY call the provided `call_llm(prompt, question)`.\n",
    "\n",
    "    ✅ Allowed:\n",
    "        - Logging, printing.\n",
    "        - Calling `call_llm()` multiple times (e.g., for retries - not implemented here).\n",
    "        - Standard string manipulations on the *response* from call_llm for validation/formatting.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The constructed prompt (instructions + context, excluding the question).\n",
    "        question (str): The original user question (to be passed to the LLM interface).\n",
    "\n",
    "    Returns:\n",
    "        str: A one-word answer: \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Executing my_call_llm...\")\n",
    "    # print(f\"  Received Prompt (Instruction + Context) length: {len(prompt)}\")\n",
    "    # print(f\"  Received Question: {question}\")\n",
    "\n",
    "    # --- Pre-computation / Checks (Optional and Allowed) ---\n",
    "    if not prompt or not question:\n",
    "        print(\"  Warning: Received empty prompt or question in my_call_llm. Returning 'Unknown'.\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # --- The Required Call to the Provided Function ---\n",
    "    print(\"  Calling the provided 'call_llm' function...\")\n",
    "    start_time = time.time()\n",
    "    # Ensure call_llm is defined in the environment this runs in\n",
    "    try:\n",
    "        raw_llm_response = call_llm(prompt, question)\n",
    "    except NameError:\n",
    "         print(\"FATAL ERROR: The required 'call_llm' function is not defined in the execution environment!\")\n",
    "         # In a real scenario, might want to raise or handle differently,\n",
    "         # but for the exercise, returning Unknown might be the safest fallback if possible.\n",
    "         return \"Unknown\" # Fallback if call_llm doesn't exist\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR during the call to 'call_llm': {e}\")\n",
    "         return \"Unknown\" # Fallback on other errors during the external call\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Inside my_call_llm, after getting raw_llm_response:\n",
    "    cleaned_response = raw_llm_response.strip().capitalize() # Keep consistent capitalization\n",
    "    final_answer: str\n",
    "    \n",
    "    # --- Prioritize Exact Matches ---\n",
    "    if cleaned_response == \"Yes\":\n",
    "        final_answer = \"Yes\"\n",
    "    elif cleaned_response == \"No\":\n",
    "        final_answer = \"No\"\n",
    "    elif cleaned_response == \"Unknown\":\n",
    "        final_answer = \"Unknown\"\n",
    "    # --- Heuristics ONLY if no exact match ---\n",
    "    elif cleaned_response.startswith(\"Yes\"):\n",
    "        final_answer = \"Yes\"\n",
    "        print(f\"  INFO: Interpreted '{cleaned_response}' as 'Yes'.\")\n",
    "    elif cleaned_response.startswith(\"No\"):\n",
    "        final_answer = \"No\"\n",
    "        print(f\"  INFO: Interpreted '{cleaned_response}' as 'No'.\")\n",
    "    # Optional: Add keyword checks for Unknown here if needed\n",
    "    elif \"not mentioned\" in raw_llm_response.lower() or \"does not say\" in raw_llm_response.lower():\n",
    "        final_answer = \"Unknown\"\n",
    "        print(f\"  INFO: Interpreted '{cleaned_response}' as 'Unknown' based on keywords.\")\n",
    "    else:\n",
    "        # Default to Unknown if no exact match and heuristics fail\n",
    "        final_answer = \"Unknown\"\n",
    "        print(f\"  WARNING: Raw response '{raw_llm_response}' -> Cleaned '{cleaned_response}' could not be mapped. Defaulting to 'Unknown'.\")\n",
    "    \n",
    "    print(f\"  Returning final validated answer: '{final_answer}'\")\n",
    "    print(\"-\" * 50)\n",
    "    return final_answer\n",
    "   "
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3a949238-121b-4db6-9e59-b965789d5a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:08.546066Z",
     "start_time": "2025-05-12T10:59:08.542337Z"
    }
   },
   "source": [
    "# DO NOT CHANGE\n",
    "def run_rag_pipeline(pdf_path,questions_path):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for all questions in the input list, printing full results and tracking execution time.\n",
    "\n",
    "    The process includes:\n",
    "    1. Loading and chunking the PDF.\n",
    "    2. Embedding and storing chunks in Qdrant.\n",
    "    3. Answering each question using a locally hosted LLM (via Ollama).\n",
    "    4. Printing the full Q&A pairs.\n",
    "    5. Reporting total runtime with a warning if the run exceeds 5 or 10 minutes.\n",
    "    6. Printing a summary of answers only (one per line).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    text = load_pdf(pdf_path)\n",
    "    questions = load_questions(questions_path)\n",
    "\n",
    "    # Chunk and store once (not inside the loop)\n",
    "    client, model = chunk_and_store(text) # your function\n",
    "\n",
    "    all_answers = []\n",
    "\n",
    "    print(\"🧠 Answering questions...\")\n",
    "    for question in questions:\n",
    "        prompt = create_prompt(question, client, model) # your function\n",
    "        answer = my_call_llm(prompt,question)\n",
    "        all_answers.append((question, answer)) \n",
    "        # print(f\"\\nQ: {prompt} \\n Q: {question} \\n A: {answer} \\n {'-'*60} \\n\")\n",
    "        # print(f\"Q: {question} \\n A: {answer} \\n {'-'*60} \\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = total_time / 60\n",
    "\n",
    "    print(\"\\n⏱️ Total Runtime: {:.2f} seconds ({:.2f} minutes)\".format(total_time, minutes))\n",
    "    if minutes > 10:\n",
    "        print(\"⚠️ Warning: Runtime exceeds 10 minutes!\")\n",
    "    elif minutes > 5:\n",
    "        print(\"⚠️ Notice: Runtime exceeds 5 minutes.\")\n",
    "\n",
    "    print(\"\\n📝 Summary of Answers:\")\n",
    "    i=0\n",
    "    for _, answer in all_answers:\n",
    "        i+=1\n",
    "        print(i,\". \",answer)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "89887587-0155-4dda-8013-6a3ec677a9b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:59:08.599788Z",
     "start_time": "2025-05-12T10:59:08.574491Z"
    }
   },
   "source": [
    "# DO NOT CHANGE\n",
    "run_rag_pipeline(PDF_PATH,QUESTIONS_PATH)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# DO NOT CHANGE\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mrun_rag_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPDF_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43mQUESTIONS_PATH\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mrun_rag_pipeline\u001B[39m\u001B[34m(pdf_path, questions_path)\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_rag_pipeline\u001B[39m(pdf_path,questions_path):\n\u001B[32m      3\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[33;03m    Runs the RAG pipeline for all questions in the input list, printing full results and tracking execution time.\u001B[39;00m\n\u001B[32m      5\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     12\u001B[39m \u001B[33;03m    6. Printing a summary of answers only (one per line).\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     start_time = \u001B[43mtime\u001B[49m.time()\n\u001B[32m     16\u001B[39m     text = load_pdf(pdf_path)\n\u001B[32m     17\u001B[39m     questions = load_questions(questions_path)\n",
      "\u001B[31mNameError\u001B[39m: name 'time' is not defined"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
