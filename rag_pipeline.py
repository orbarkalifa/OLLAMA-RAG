import fitz
import os
import uuid
import subprocess
from langchain.text_splitter import (
    CharacterTextSplitter,
    NLTKTextSplitter,
    SpacyTextSplitter,
    RecursiveCharacterTextSplitter
)
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance, PointStruct
import time

# SET PATH According to your configuration.
PDF_PATH = "MyBank Credit Card Brochure.pdf"
QUESTIONS_PATH = "questions.txt"

# DO NOT CHANGE
def load_pdf(pdf_path):
    with fitz.open(pdf_path) as doc:
        text = "\n".join(page.get_text() for page in doc)
    return text

# DO NOT CHANGE
def load_questions(questions_path):
    with open(questions_path, "r") as f:
        questions = [q.strip() for q in f.readlines() if q.strip().endswith('?')]
    return questions


# DO NOT CHANGE
# LLM via Ollama
def call_llm(prompt: str, question: str) -> str:
    """
    Calls a local LLM using the Ollama CLI and returns the model's response.

    This function sends a prompt to the locally hosted `llama3.2:3b` model via the `ollama` command-line interface.
    It ensures the prompt does not exceed 500 characters and captures the model's output.

    Parameters:
        prompt (str): The full input prompt to be sent to the LLM. It should include context and instructions,
                      but not the question itself if using external control.

    Returns:
        str: The raw response generated by the model. If the model call times out, returns "Unknown".

    Notes:
        - The prompt is truncated to a maximum of 2000 characters before being sent.
        - The model is expected to return a one-word answer such as "Yes", "No", or "Unknown".
    """
    prompt = prompt[:2000] + "\nQuestion: " + question
    try:
        result = subprocess.run(
            ["ollama", "run", "llama3.2:3b"],
            input=prompt.encode("utf-8"),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=30
        )
        return result.stdout.decode("utf-8").strip()
    except subprocess.TimeoutExpired:
        return "Unknown"


def chunk_and_store(text):
    """
    1. Split `text` into overlapping 300-char chunks (50-char overlap).
    2. Embed each chunk with all-MiniLM-L6-v2.
    3. Create an in-memory Qdrant collection and upsert all vectors + metadata.
    """
    # 1. chunking
    WINDOW_SIZE, OVERLAP = 300, 50
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + WINDOW_SIZE, len(text))
        chunks.append((text[start:end], start, end))
        start += WINDOW_SIZE - OVERLAP

    print("[DEBUG] First 3 raw chunks:")
    for chunk, s, e in chunks[:3]:
        print(f"  ‚Äì chars {s}:{e} -> {repr(chunk[:50])}‚Ä¶")

    # 2. embed
    model = SentenceTransformer("all-MiniLM-L6-v2")
    vector_size = model.get_sentence_embedding_dimension()

    # 3. qdrant setup
    client = QdrantClient(":memory:")
    client.recreate_collection(
        collection_name="pdf_chunks",
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
    )

    points = []
    for idx, (chunk, s, e) in enumerate(chunks):
        vec = model.encode(chunk).tolist()
        payload = {"text": chunk, "start": s, "end": e}
        points.append(PointStruct(id=idx, vector=vec, payload=payload))

    client.upsert(collection_name="pdf_chunks", points=points)
    count = client.count(collection_name="pdf_chunks").count
    print(f"[DEBUG] Stored {count} chunks in Qdrant")
    return client, model


def create_prompt(question, client, model):
    """
    1. Embed the question.
    2. Retrieve top-5 chunks from Qdrant.
    3. Prepend a fixed instruction and concatenate truncated snippets.
    4. If >400 chars (excluding the question), drop earliest snippets until it fits.
    """
    q_vec = model.encode(question).tolist()
    hits = client.search(
        collection_name="pdf_chunks",
        query_vector=q_vec,
        limit=5
    )

    print(f"[DEBUG] Top-{len(hits)} hits for question ‚Äú{question}‚Äù:")
    for i, h in enumerate(hits):
        txt = h.payload["text"].replace("\n", " ")[:60]
        print(f"  {i+1}. id={h.id}, score={h.score:.4f}, text={repr(txt)}‚Ä¶")

    instruction = (
        "You are given relevant passages from a PDF. "
        "Answer Yes, No, or Unknown based ONLY on these. "
        "Do NOT use any outside knowledge.\n\n"
    )

    # truncate each hit to its first 150 chars so at least one survives the cap
    raw_texts = [h.payload["text"] for h in hits]
    context_blocks = [txt[:150] for txt in raw_texts]

    prompt = instruction + "\n\n---\n\n".join(context_blocks)

    # enforce 400-char limit
    while len(prompt) > 400 and len(context_blocks) > 1:
        context_blocks.pop()  # drop the least relevant snippet
        prompt = instruction + "\n\n---\n\n".join(context_blocks)

    print(f"[DEBUG] Final prompt ({len(prompt)} chars):\n{prompt!r}\n")
    return prompt


def my_call_llm(prompt, question):
    """
    Thin wrapper around the provided call_llm().
    Ensures we only return "Yes", "No", or "Unknown".
    """
    raw = call_llm(prompt, question)
    ans = raw.strip().capitalize()
    if ans not in {"Yes", "No", "Unknown"}:
        ans = "Unknown"
    return ans

# DO NOT CHANGE
def run_rag_pipeline(pdf_path,questions_path):
    """
    Runs the RAG pipeline for all questions in the input list, printing full results and tracking execution time.

    The process includes:
    1. Loading and chunking the PDF.
    2. Embedding and storing chunks in Qdrant.
    3. Answering each question using a locally hosted LLM (via Ollama).
    4. Printing the full Q&A pairs.
    5. Reporting total runtime with a warning if the run exceeds 5 or 10 minutes.
    6. Printing a summary of answers only (one per line).
    """
    start_time = time.time()

    text = load_pdf(pdf_path)
    questions = load_questions(questions_path)

    # Chunk and store once (not inside the loop)
    client, model = chunk_and_store(text)

    all_answers = []

    print("üß† Answering questions...")
    for question in questions:
        prompt = create_prompt(question, client, model)
        answer = my_call_llm(prompt,question)
        all_answers.append((question, answer))

    total_time = time.time() - start_time
    minutes = total_time / 60

    print("\n‚è±Ô∏è Total Runtime: {:.2f} seconds ({:.2f} minutes)".format(total_time, minutes))
    if minutes > 10:
        print("‚ö†Ô∏è Warning: Runtime exceeds 10 minutes!")
    elif minutes > 5:
        print("‚ö†Ô∏è Notice: Runtime exceeds 5 minutes.")

    print("\nüìù Summary of Answers:")
    for i, (_, answer) in enumerate(all_answers, start=1):
        print(i, ". ", answer)

# DO NOT CHANGE
run_rag_pipeline(PDF_PATH,QUESTIONS_PATH)
