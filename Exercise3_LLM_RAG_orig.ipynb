{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5434f62b",
   "metadata": {},
   "source": [
    "# üß™ Exercise 3: PDF Question Answering using Chunking, Vector Search, and LLM\n",
    "\n",
    "In this exercise, you'll complete a **retrieval-augmented generation (RAG)** pipeline that:\n",
    "- Chunks and embeds the content of a PDF\n",
    "- Stores the chunks in an in-memory vector database (Qdrant)\n",
    "- Uses a local LLM to answer yes/no/unknown questions based strictly on the PDF content\n",
    "\n",
    "You will implement the missing components of this pipeline, focusing on document chunking, retrieval, and prompt construction.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "Your objective is to build a system that can answer **yes**, **no**, or **unknown** questions based solely on the information in a given **PDF file**.\n",
    "\n",
    "- The answer must be **one word only**: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`\n",
    "- The LLM must not use external knowledge ‚Äî it must rely **only** on content retrieved from the PDF\n",
    "- The total prompt sent to the LLM is limited to **2000 characters**, including:\n",
    "  - The instruction\n",
    "  - Retrieved context chunks  \n",
    "  *(üö´ The question itself is not included in this limit and will be added separately)*\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What You Need to Do\n",
    "\n",
    "You must implement the following three core functions:\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ `chunk_and_store(text: str) -> tuple[QdrantClient, SentenceTransformer]`\n",
    "\n",
    "This function prepares the document for retrieval.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Chunk the input `text` into smaller segments\n",
    "- Encode each chunk into a vector using a pre-trained embedding model such as `all-MiniLM-L6-v2`\n",
    "- Store the vectors in an in-memory **Qdrant** database using `QdrantClient(\":memory:\")`\n",
    "- Store relevant metadata for each chunk (e.g., start offset, method)\n",
    "\n",
    "**Returns:**\n",
    "- `client`: a Qdrant client object that contains the embedded chunks\n",
    "- `model`: the SentenceTransformer model used for encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ `create_prompt(question: str, client: QdrantClient, model: SentenceTransformer) -> str`\n",
    "\n",
    "This function builds the prompt to be sent to the LLM.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Retrieve the top-k most relevant chunks from Qdrant using the question as a query\n",
    "- Construct a prompt that includes:\n",
    "  - A fixed instruction (you may define this in the function)\n",
    "  - The most relevant retrieved chunks\n",
    "- The full prompt must not exceed **400 characters**, excluding the question\n",
    "\n",
    "**Returns:**\n",
    "- A string containing the prompt (instruction + context), **excluding** the question\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ‚úÖ `my_call_llm(prompt: str, question: str) -> str`\n",
    "\n",
    "This function provides an interface to the LLM, but must not invoke the LLM directly.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Optionally apply logic to enhance or adapt the query (e.g., pre-processing the prompt, logging, enforcing formatting rules)\n",
    "- Call the provided `call_llm(prompt, question)` function to actually interact with the model\n",
    "- Return the result unchanged, or with controlled, explainable adjustments **that do not modify the content of the LLM‚Äôs response**\n",
    "\n",
    "**Rules:**\n",
    "- ‚ùå Must **not** embed, re-embed, or analyze any part of the original document or its chunks\n",
    "- ‚ùå Must **not** call `subprocess`, `ollama`, or any direct LLM API\n",
    "- ‚úÖ Must **only** call `call_llm(prompt, question)` to obtain the response\n",
    "\n",
    "**Returns:**\n",
    "- A string (typically `\"Yes\"`, `\"No\"`, or `\"Unknown\"`) returned by the LLM, possibly post-processed for stability, format, or logging\n",
    "\n",
    "**Purpose:**\n",
    "- This function acts as a controlled gateway to LLM usage, allowing improvements in how prompts are used or tracked, without modifying or reprocessing the document or query logic\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> üí° **Tip:** You are encouraged to define helper functions to simplify your code and improve readability.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Provided Function (DO NOT CHANGE)\n",
    "\n",
    "#### ‚úÖ `call_llm(prompt: str) -> str`\n",
    "\n",
    "This function is already implemented for you.\n",
    "\n",
    "- It calls the `llama3.2:3b` model via `ollama`\n",
    "- Receives the question and your constructed prompt\n",
    "- Returns the LLM's answer (expected: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`)\n",
    "\n",
    "You do **not** need to re-implement or modify this function.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Evaluation Criteria\n",
    "\n",
    "- Your system will be evaluated using a **corpus of 100 questions** on a **known PDF document**\n",
    "- You will be given in advance a **sample of 20 questions** from the evaluation corpus for development and testing\n",
    "- Your code must generate the correct **yes/no/unknown** answers for the full 100-question set\n",
    "- **Total execution time** will be measured for the entire run (reading, chunking, querying, and answering)\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Restrictions\n",
    "\n",
    "- **Do not** modify any code cell marked with `# DO NOT CHANGE`.\n",
    "- **Do not** override any variable or function defined in those protected cells.\n",
    "- Your code must run successfully in the **Lab 10002** environment (`GenAI025_CUDA`), using only the libraries provided.\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Tip:** Write clean, modular code. Aim for accuracy, clarity, and runtime efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Good luck!\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Points Deduction Rules\n",
    "\n",
    "1. **Modifying restricted code**  \n",
    "   - Changing any `# DO NOT CHANGE` cell or variable: **‚Äì50 points**\n",
    "\n",
    "2. **Importing any additional library**  \n",
    "   - Importing any library that is **not already used** in the template: **‚Äì5 points per library**  \n",
    "   - ‚úÖ *No penalty* for importing additional modules or functions from libraries that are already used (e.g., importing more from `langchain` or `sentence_transformers`)\n",
    "\n",
    "3. **Code compatibility**  \n",
    "   - Code fails to run in Lab 10002: **‚Äì100 points**\n",
    "\n",
    "4. **Execution time (total run of 100 questions)**  \n",
    "   - Runs for **5‚Äì10 minutes**: **‚Äì30 points**  \n",
    "   - Runs for **>10 minutes**: **‚Äì100 points**\n",
    "\n",
    "5.  **Violating restrictions inside `my_call_llm()`**  \n",
    "   - ‚ùå Must **not** embed, re-embed, or analyze any part of the original document or its chunks  \n",
    "   - ‚ùå Must **not** call `subprocess`, `ollama`, or any direct LLM API  \n",
    "   - ‚úÖ Must **only** call `call_llm(prompt, question)` to obtain the response  \n",
    "   - Penalty: **‚Äì100 points**\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Final Score Calculation\n",
    "\n",
    "$$\n",
    "\\text{Final Score} = \\min \\left(100,\\ \\frac{\\text{Your correct answers}}{\\text{Gadi‚Äôs correct answers}} \\times 100 \\right) - \\text{Total Deductions}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üìå *Submit clean, working code. Only modify what you're allowed to. You got this!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d495b5d6-7594-41b4-a568-1fe3e971acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PATH According to your configuration.\n",
    "PDF_PATH = \"MyBank Credit Card Brochure.pdf\"\n",
    "QUESTIONS_PATH = \"questions.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afada56a-bf9e-4f76-a4fa-ad69f8e43b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "import fitz\n",
    "import os\n",
    "import uuid\n",
    "import spacy\n",
    "import subprocess\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    NLTKTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, PointStruct\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d21727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "def load_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e838a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "def load_questions(questions_path):\n",
    "    with open(questions_path, \"r\") as f:\n",
    "        questions = [q.strip() for q in f.readlines() if q.strip().endswith('?')]\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7505de-7771-494c-b623-1cf525e0fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS AND GLOBAL VARIABLES\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Model and Collection ---\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "# Define a FIXED collection name that both chunk_and_store and create_prompt will use\n",
    "COLLECTION_NAME = \"rag_exercise_pdf_collection_fixed_v1\" # Using a specific, fixed name\n",
    "\n",
    "# --- Chunking Parameters ---\n",
    "CHUNK_METHOD = \"RecursiveCharacterTextSplitter\"\n",
    "CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 40\n",
    "\n",
    "# --- Retrieval and Prompting Parameters ---\n",
    "TOP_K = 3                            # Number of top relevant chunks to retrieve - TUNE THIS\n",
    "PROMPT_CONTEXT_LIMIT = 400           # Max characters for the final prompt (instruction + context)\n",
    "\n",
    "\n",
    "# --- Helper Function: find_chunks ---\n",
    "def find_chunks(question: str, client: QdrantClient, model: SentenceTransformer, collection_name: str, top_k: int = TOP_K):\n",
    "    \"\"\"\n",
    "    Encodes the question and searches Qdrant for the top_k most relevant chunks.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        client: Initialized QdrantClient.\n",
    "        model: Initialized SentenceTransformer model.\n",
    "        collection_name: The name of the Qdrant collection to search.\n",
    "        top_k: The maximum number of chunks to retrieve (defaults to global TOP_K).\n",
    "\n",
    "    Returns:\n",
    "        A list of Qdrant ScoredPoint objects representing the relevant chunks.\n",
    "        Returns an empty list if the search fails or yields no results.\n",
    "    \"\"\"\n",
    "    if not collection_name:\n",
    "        print(\"Error: Collection name is not set for find_chunks.\")\n",
    "        # Consider raising an error instead of just printing and returning empty list\n",
    "        # raise ValueError(\"Collection name must be provided to find_chunks\")\n",
    "        return []\n",
    "\n",
    "    # Use the provided top_k argument, which defaults to the global TOP_K\n",
    "    print(f\"  Searching for top {top_k} chunks in collection '{collection_name}'...\")\n",
    "    try:\n",
    "        # 1. Embed the Question\n",
    "        question_vector = model.encode(question).tolist()\n",
    "\n",
    "        # 2. Search Qdrant\n",
    "        search_results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=question_vector,\n",
    "            limit=top_k\n",
    "            # Optional: Add score_threshold=0.X if needed based on experimentation\n",
    "            # score_threshold=0.4 # Example threshold\n",
    "        )\n",
    "        print(f\"  Found {len(search_results)} candidate chunks.\")\n",
    "        return search_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during Qdrant search in find_chunks: {e}\")\n",
    "        # Consider re-raising the exception or logging it more formally\n",
    "        return [] # Return empty list on error\n",
    "\n",
    "# --- Type Hint Imports (Make sure these are at the top of the file) ---\n",
    "# from typing import List\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.http.models import ScoredPoint\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import uuid # If generating collection names dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f47aeb",
   "metadata": {},
   "source": [
    "### ‚úÖ Task 1: Implement `chunk_and_store(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b8a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_store(text: str):\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller chunks and stores them in a vector database or an internal memory structure.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed. This should be a large block of text (e.g., a document, an article, or a report).\n",
    "\n",
    "    Behavior:\n",
    "    --------\n",
    "    1. The function splits the input `text` into manageable chunks based on predefined chunking rules \n",
    "       (e.g., maximum character count, sentence boundaries, semantic meaning).\n",
    "    2. Each chunk is optionally enriched with metadata (e.g., chunk number, character offsets, original document ID).\n",
    "    3. Each chunk is stored in a storage system such as:\n",
    "       - An in-memory list or dictionary (for simple setups)\n",
    "       - A vector database (e.g., Qdrant, FAISS, ChromaDB) after embedding the chunk using an encoder model\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    client : qdrant_client.QdrantClient\n",
    "        A Qdrant client object that contains the embedded and stored chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used for embedding the text chunks.\n",
    "   \n",
    "    Notes:\n",
    "    -----\n",
    "    - If using a vector database, the chunk is first passed through an embedding model to create a vector representation.\n",
    "    - Chunking methods might vary (e.g., fixed-size, sentence-based, semantic-split) depending on implementation details.\n",
    "    - The function assumes that the storage backend is already initialized and ready for storing chunks.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `text` is empty or not a valid string.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> chunk_and_store(\"This is a long article about machine learning...\")\n",
    "    # Splits the article into chunks and stores them internally or externally.\n",
    "\n",
    "    \"\"\"\n",
    "    # Implementation goes here\n",
    "    # TODO: implement chunking using multiple strategies\n",
    "    # TODO: create in-memory Qdrant collection\n",
    "    # TODO: embed each chunk and store in the DB with metadata (chunking method, start_offset)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Splits text into chunks using RecursiveCharacterTextSplitter, embeds them,\n",
    "    and stores them in an in-memory Qdrant vector database using the globally\n",
    "    defined COLLECTION_NAME.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed (e.g., content of a PDF).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    client : qdrant_client.QdrantClient\n",
    "        A Qdrant client object connected to the in-memory database containing the embedded chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used for embedding the text chunks.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `text` is empty or not a valid string.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(\"Input text cannot be empty and must be a string.\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Starting chunking and storing process...\")\n",
    "    print(f\"Input text length: {len(text)} characters\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Initialize Embedding Model\n",
    "    print(f\"Loading sentence transformer model: {MODEL_NAME}\")\n",
    "    try:\n",
    "        model = SentenceTransformer(MODEL_NAME)\n",
    "        embedding_size = model.get_sentence_embedding_dimension()\n",
    "        if embedding_size is None:\n",
    "             raise ValueError(\"Could not determine embedding dimension from the model.\")\n",
    "        print(f\"Model loaded successfully. Embedding dimension: {embedding_size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer model '{MODEL_NAME}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # 2. Initialize Qdrant Client (In-Memory)\n",
    "    print(\"Initializing in-memory Qdrant client...\")\n",
    "    try:\n",
    "        client = QdrantClient(\":memory:\")\n",
    "        print(\"Qdrant client initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Qdrant client: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 3. Create Qdrant Collection using the global COLLECTION_NAME\n",
    "    print(f\"Creating or recreating Qdrant collection: '{COLLECTION_NAME}'\")\n",
    "    try:\n",
    "        client.recreate_collection(\n",
    "            collection_name=COLLECTION_NAME, # Use the global constant\n",
    "            vectors_config=models.VectorParams(size=embedding_size, distance=models.Distance.COSINE)\n",
    "        )\n",
    "        print(f\"Collection '{COLLECTION_NAME}' created/recreated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/recreating Qdrant collection '{COLLECTION_NAME}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # 4. Chunk the Text using the chosen strategy (defined globally)\n",
    "    print(f\"Chunking text using {CHUNK_METHOD} (Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP})\")\n",
    "    try:\n",
    "        if CHUNK_METHOD == \"RecursiveCharacterTextSplitter\":\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                length_function=len,\n",
    "            )\n",
    "        else:\n",
    "            # Add other methods here if needed, referencing CHUNK_METHOD\n",
    "            raise ValueError(f\"Unsupported chunking method specified globally: {CHUNK_METHOD}\")\n",
    "\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        print(f\"Text split into {len(chunks)} chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text chunking: {e}\")\n",
    "        raise\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"Warning: No chunks were generated from the text. Returning empty client.\")\n",
    "        return client, model\n",
    "\n",
    "    # 5. Embed Chunks and Prepare Points for Qdrant\n",
    "    print(\"Embedding chunks and preparing points for Qdrant...\")\n",
    "    points_to_upsert = []\n",
    "    chunk_processing_start_time = time.time()\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        if not chunk_text.strip():\n",
    "            print(f\"Skipping empty chunk at index {i}\")\n",
    "            continue\n",
    "        try:\n",
    "            vector = model.encode(chunk_text).tolist()\n",
    "            payload = { \"text\": chunk_text, \"chunk_index\": i, \"method\": CHUNK_METHOD }\n",
    "            # Use a deterministic or random UUID for point IDs\n",
    "            point_id = str(uuid.uuid4()) # Use random UUID\n",
    "            # Or use deterministic ID if needed:\n",
    "            # point_id = uuid.uuid5(uuid.NAMESPACE_DNS, f'{COLLECTION_NAME}_{i}_{chunk_text[:20]}').hex\n",
    "\n",
    "            point = PointStruct(id=point_id, vector=vector, payload=payload)\n",
    "            points_to_upsert.append(point)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing or embedding chunk {i}: {e}\")\n",
    "            raise\n",
    "\n",
    "    chunk_processing_end_time = time.time()\n",
    "    print(f\"Embedding and point preparation took {chunk_processing_end_time - chunk_processing_start_time:.2f} seconds.\")\n",
    "\n",
    "    # 6. Upsert Points to Qdrant Collection using the global COLLECTION_NAME\n",
    "    if points_to_upsert:\n",
    "        print(f\"Upserting {len(points_to_upsert)} points into collection '{COLLECTION_NAME}'...\")\n",
    "        try:\n",
    "            client.upsert(\n",
    "                collection_name=COLLECTION_NAME, # Use the global constant\n",
    "                points=points_to_upsert,\n",
    "                wait=True\n",
    "            )\n",
    "            print(\"Upsert operation successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting points into Qdrant: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(\"No valid points were generated to upsert.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Chunking and storing completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 7. Return only the client and model, as per original signature\n",
    "    return client, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169634c",
   "metadata": {},
   "source": [
    "### ‚úÖ Task 2: Implement `create_prompt(question, client, model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e771f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question: str, client, model):\n",
    "    \"\"\"\n",
    "    Creates a context-only prompt for an LLM by retrieving relevant chunks from a vector database \n",
    "    based on a user question, using a vector similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    question : str\n",
    "        The input question provided by the user. It should be a natural language query.\n",
    "\n",
    "    client : qdrant_client.QdrantClient\n",
    "        The Qdrant client connected to the database that contains stored and embedded text chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used to encode the input question into a vector embedding \n",
    "        for similarity search.\n",
    "\n",
    "    Behavior:\n",
    "    --------\n",
    "    1. The function encodes the input `question` into a vector using the provided `model`.\n",
    "    2. It queries the `client` (Qdrant database) using vector similarity search to find the most relevant chunks.\n",
    "    3. It assembles a prompt by combining the retrieved chunks and other info (but without adding the question itself).\n",
    "    4. The resulting prompt consists **only of context**, intended to be passed separately along with the question \n",
    "       in a later step when calling the LLM.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    prompt : str\n",
    "        A fully formatted prompt string. \n",
    "        **The user's question is NOT included in the returned prompt.**\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The search typically retrieves the top-k most similar chunks (e.g., top 5).\n",
    "    - Retrieved chunks are usually concatenated together, separated by delimiters (e.g., \"\\n\\n\").\n",
    "    - The question should be provided separately to the LLM after sending the prompt, or combined externally later.\n",
    "    - This function assumes that both the client and model are already initialized and ready to use.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `question` is empty or not a valid string.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> context_prompt = create_prompt(\"What benefits does the Platinum Voyager Card offer?\", client, model)\n",
    "    >>> print(context_prompt)\n",
    "    \"Context:\\n<retrieved chunks>\"\n",
    "\n",
    "    # Later, when calling the LLM:\n",
    "    # final_prompt = context_prompt + \"\\n\\nQuestion:\\nWhat benefits does the Platinum Voyager Card offer?\"\n",
    "    \"\"\"\n",
    "    # TODO: use find_chunks()\n",
    "    # TODO: build the prompt with CONTEXT_HEADER and top chunks\n",
    "    # TODO: truncate to PROMPT_CHAR_LIMIT if needed\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a context-only prompt for an LLM by retrieving relevant chunks from\n",
    "    the globally defined COLLECTION_NAME in Qdrant, based on a user question,\n",
    "    adhering to character limits.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    question : str\n",
    "        The input question provided by the user.\n",
    "    client : qdrant_client.QdrantClient\n",
    "        The Qdrant client connected to the database with embedded chunks.\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model for encoding the question.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    prompt : str\n",
    "        A formatted prompt string containing instructions and retrieved context,\n",
    "        NOT including the original question. The total length is capped by\n",
    "        PROMPT_CONTEXT_LIMIT.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `question` is empty or not a valid string.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Creating prompt for question: '{question[:100]}...'\")\n",
    "\n",
    "    if not question or not isinstance(question, str):\n",
    "        raise ValueError(\"Input question cannot be empty and must be a string.\")\n",
    "\n",
    "    # 1. Define the Strict Instruction for the LLM\n",
    "    instruction = (\n",
    "    \"Answer using ONLY the Context below. Respond with 'Yes', 'No', or 'Unknown'. \"\n",
    "    \"If context is insufficient, answer 'Unknown'.\\n\\n\"\n",
    "    \"Context:\\n---\\n\"\n",
    "    )\n",
    "    context_end_delimiter = \"\\n---\"\n",
    "\n",
    "    # 2. Find Relevant Chunks directly using the global COLLECTION_NAME\n",
    "    print(f\"Finding relevant chunks in fixed collection: {COLLECTION_NAME} using TOP_K={TOP_K}...\")\n",
    "    try:\n",
    "        question_vector = model.encode(question).tolist()\n",
    "        relevant_chunks: List[ScoredPoint] = client.search(\n",
    "            collection_name=COLLECTION_NAME, # Use global constant directly\n",
    "            query_vector=question_vector,\n",
    "            limit=TOP_K # Use global constant\n",
    "            # Optional: score_threshold=0.4 # Add if tuning improves results\n",
    "        )\n",
    "        print(f\"  Found {len(relevant_chunks)} candidate chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during Qdrant search in create_prompt: {e}\")\n",
    "        relevant_chunks = [] # Continue with empty context on search error\n",
    "\n",
    "    # 3. Build Context String, Respecting Character Limit (PROMPT_CONTEXT_LIMIT)\n",
    "    print(f\"Building context string, respecting limit of {PROMPT_CONTEXT_LIMIT} chars for instruction + context...\")\n",
    "    context_parts = []\n",
    "    # Calculate length of fixed parts (instruction + final delimiter)\n",
    "    current_length = len(instruction) + len(context_end_delimiter)\n",
    "    added_chunks_count = 0\n",
    "\n",
    "    if not relevant_chunks:\n",
    "        print(\"  No relevant chunks found or retrieved.\")\n",
    "    else:\n",
    "        for i, hit in enumerate(relevant_chunks):\n",
    "            chunk_text = hit.payload.get(\"text\", \"\").strip() # Get text and strip whitespace\n",
    "            if not chunk_text:\n",
    "                print(f\"  Skipping empty chunk from search result {i}.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate length needed for this chunk (+1 for newline separator if not the first chunk)\n",
    "            length_needed = len(chunk_text) + (1 if context_parts else 0)\n",
    "\n",
    "            print(f\"  Considering chunk {i} (Score: {hit.score:.4f}, Length: {len(chunk_text)}). Needs {length_needed} chars.\")\n",
    "\n",
    "            if current_length + length_needed <= PROMPT_CONTEXT_LIMIT:\n",
    "                context_parts.append(chunk_text)\n",
    "                current_length += length_needed\n",
    "                added_chunks_count += 1\n",
    "                print(f\"    Added chunk {i}. Current prompt length: {current_length}/{PROMPT_CONTEXT_LIMIT}\")\n",
    "            else:\n",
    "                print(f\"    Skipped chunk {i}. Adding it would exceed limit ({current_length + length_needed}/{PROMPT_CONTEXT_LIMIT}). Stopping context assembly.\")\n",
    "                break # Stop adding chunks once limit is reached\n",
    "\n",
    "    # 4. Assemble the Final Prompt\n",
    "    context_string = \"\\n\".join(context_parts) # Join collected chunks with newlines\n",
    "\n",
    "    # Combine instruction, context (if any), and end delimiter\n",
    "    final_prompt = instruction + context_string + context_end_delimiter\n",
    "    final_length = len(final_prompt)\n",
    "\n",
    "    print(f\"Final prompt created. Added {added_chunks_count} chunks.\")\n",
    "    print(f\"Final prompt length (instruction + context): {final_length} characters.\")\n",
    "\n",
    "    # Safeguard: Ensure the limit wasn't somehow exceeded (shouldn't happen with the logic above)\n",
    "    if final_length > PROMPT_CONTEXT_LIMIT:\n",
    "        print(f\"ERROR: Final prompt length ({final_length}) exceeded limit ({PROMPT_CONTEXT_LIMIT}). Truncating brutally.\")\n",
    "        # Fallback: truncate the context part to fit.\n",
    "        excess = final_length - PROMPT_CONTEXT_LIMIT\n",
    "        # Ensure context_string has enough characters to remove 'excess' amount\n",
    "        if len(context_string) >= excess:\n",
    "             truncated_context_string = context_string[:-excess]\n",
    "        else:\n",
    "             # If context is somehow shorter than the excess (edge case), empty it\n",
    "             truncated_context_string = \"\"\n",
    "        final_prompt = instruction + truncated_context_string + context_end_delimiter\n",
    "        print(f\"Truncated prompt length: {len(final_prompt)}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    return final_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b50937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# LLM via Ollama\n",
    "def call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls a local LLM using the Ollama CLI and returns the model's response.\n",
    "\n",
    "    This function sends a prompt to the locally hosted `llama3.2:3b` model via the `ollama` command-line interface.\n",
    "    It ensures the prompt does not exceed 500 characters and captures the model's output.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The full input prompt to be sent to the LLM. It should include context and instructions,\n",
    "                      but not the question itself if using external control.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw response generated by the model. If the model call times out, returns \"Unknown\".\n",
    "\n",
    "    Notes:\n",
    "        - The prompt is truncated to a maximum of 2000 characters before being sent.\n",
    "        - The model is expected to return a one-word answer such as \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    prompt = prompt[:2000] + \"\\nQuestion: \" + question\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3.2:3b\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=30\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd328aed-0af0-4800-bdb4-89c00011f208",
   "metadata": {},
   "source": [
    "### ‚úÖ Task 3: Implement `my_call_llm(prompt: str, question: str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7fe044-5ed7-4c33-bb1b-d71d87498a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    A wrapper function for controlled interaction with the local LLM.\n",
    "\n",
    "    This function allows for preprocessing, logging, or evaluation logic\n",
    "    around a call to the provided `call_llm()` function, but it must not\n",
    "    directly interact with the LLM (e.g., via subprocess or embedding logic).\n",
    "\n",
    "    üö´ Restrictions:\n",
    "        - Must NOT embed, re-embed, or analyze any part of the original document or its chunks\n",
    "        - Must NOT call `subprocess`, `ollama`, or any direct LLM APIs\n",
    "        - Must ONLY interact with the LLM via the provided `call_llm(prompt, question)` function\n",
    "\n",
    "    ‚úÖ Allowed:\n",
    "        - Logging or printing\n",
    "        - Handling empty prompts or question formats\n",
    "        - Calling `call_llm()` multiple times for retry logic or consistency checking\n",
    "        - Standard string manipulations (if needed)\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The constructed prompt (instructions + context, excluding the question).\n",
    "        question (str): The original user question (to be passed to the LLM interface).\n",
    "\n",
    "    Returns:\n",
    "        str: - return a one-word answer typically one of \"Yes\", \"No\", or \"Unknown\".\n",
    "\n",
    "    Example:\n",
    "        >>> my_call_llm(\"Context: Data is collected by Google...\", \"Does Google share my location?\")\n",
    "        \"Yes\"\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    A wrapper function for controlled interaction with the local LLM via the\n",
    "    provided call_llm function, ensuring the final output is one of \"Yes\", \"No\", or \"Unknown\".\n",
    "\n",
    "    üö´ Restrictions:\n",
    "        - Must NOT embed, re-embed, or analyze document content or chunks.\n",
    "        - Must NOT call subprocess, ollama, or any direct LLM API.\n",
    "        - Must ONLY call the provided `call_llm(prompt, question)`.\n",
    "\n",
    "    ‚úÖ Allowed:\n",
    "        - Logging, printing.\n",
    "        - Calling `call_llm()` multiple times (e.g., for retries - not implemented here).\n",
    "        - Standard string manipulations on the *response* from call_llm for validation/formatting.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The constructed prompt (instructions + context, excluding the question).\n",
    "        question (str): The original user question (to be passed to the LLM interface).\n",
    "\n",
    "    Returns:\n",
    "        str: A one-word answer: \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Executing my_call_llm...\")\n",
    "    # print(f\"  Received Prompt (Instruction + Context) length: {len(prompt)}\")\n",
    "    # print(f\"  Received Question: {question}\")\n",
    "\n",
    "    # --- Pre-computation / Checks (Optional and Allowed) ---\n",
    "    if not prompt or not question:\n",
    "        print(\"  Warning: Received empty prompt or question in my_call_llm. Returning 'Unknown'.\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # --- The Required Call to the Provided Function ---\n",
    "    print(\"  Calling the provided 'call_llm' function...\")\n",
    "    start_time = time.time()\n",
    "    # Ensure call_llm is defined in the environment this runs in\n",
    "    try:\n",
    "        raw_llm_response = call_llm(prompt, question)\n",
    "    except NameError:\n",
    "         print(\"FATAL ERROR: The required 'call_llm' function is not defined in the execution environment!\")\n",
    "         # In a real scenario, might want to raise or handle differently,\n",
    "         # but for the exercise, returning Unknown might be the safest fallback if possible.\n",
    "         return \"Unknown\" # Fallback if call_llm doesn't exist\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR during the call to 'call_llm': {e}\")\n",
    "         return \"Unknown\" # Fallback on other errors during the external call\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Inside my_call_llm, after getting raw_llm_response:\n",
    "    cleaned_response = raw_llm_response.strip().capitalize() # Keep consistent capitalization\n",
    "    final_answer: str\n",
    "    \n",
    "    # --- Prioritize Exact Matches ---\n",
    "    if cleaned_response == \"Yes\":\n",
    "        final_answer = \"Yes\"\n",
    "    elif cleaned_response == \"No\":\n",
    "        final_answer = \"No\"\n",
    "    elif cleaned_response == \"Unknown\":\n",
    "        final_answer = \"Unknown\"\n",
    "    # --- Heuristics ONLY if no exact match ---\n",
    "    elif cleaned_response.startswith(\"Yes\"):\n",
    "        final_answer = \"Yes\"\n",
    "        print(f\"  INFO: Interpreted '{cleaned_response}' as 'Yes'.\")\n",
    "    elif cleaned_response.startswith(\"No\"):\n",
    "        final_answer = \"No\"\n",
    "        print(f\"  INFO: Interpreted '{cleaned_response}' as 'No'.\")\n",
    "    # Optional: Add keyword checks for Unknown here if needed\n",
    "    elif \"not mentioned\" in raw_llm_response.lower() or \"does not say\" in raw_llm_response.lower():\n",
    "        final_answer = \"Unknown\"\n",
    "        print(f\"  INFO: Interpreted '{cleaned_response}' as 'Unknown' based on keywords.\")\n",
    "    else:\n",
    "        # Default to Unknown if no exact match and heuristics fail\n",
    "        final_answer = \"Unknown\"\n",
    "        print(f\"  WARNING: Raw response '{raw_llm_response}' -> Cleaned '{cleaned_response}' could not be mapped. Defaulting to 'Unknown'.\")\n",
    "    \n",
    "    print(f\"  Returning final validated answer: '{final_answer}'\")\n",
    "    print(\"-\" * 50)\n",
    "    return final_answer\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a949238-121b-4db6-9e59-b965789d5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "def run_rag_pipeline(pdf_path,questions_path):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for all questions in the input list, printing full results and tracking execution time.\n",
    "\n",
    "    The process includes:\n",
    "    1. Loading and chunking the PDF.\n",
    "    2. Embedding and storing chunks in Qdrant.\n",
    "    3. Answering each question using a locally hosted LLM (via Ollama).\n",
    "    4. Printing the full Q&A pairs.\n",
    "    5. Reporting total runtime with a warning if the run exceeds 5 or 10 minutes.\n",
    "    6. Printing a summary of answers only (one per line).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    text = load_pdf(pdf_path)\n",
    "    questions = load_questions(questions_path)\n",
    "\n",
    "    # Chunk and store once (not inside the loop)\n",
    "    client, model = chunk_and_store(text) # your function\n",
    "\n",
    "    all_answers = []\n",
    "\n",
    "    print(\"üß† Answering questions...\")\n",
    "    for question in questions:\n",
    "        prompt = create_prompt(question, client, model) # your function\n",
    "        answer = my_call_llm(prompt,question)\n",
    "        all_answers.append((question, answer)) \n",
    "        # print(f\"\\nQ: {prompt} \\n Q: {question} \\n A: {answer} \\n {'-'*60} \\n\")\n",
    "        # print(f\"Q: {question} \\n A: {answer} \\n {'-'*60} \\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = total_time / 60\n",
    "\n",
    "    print(\"\\n‚è±Ô∏è Total Runtime: {:.2f} seconds ({:.2f} minutes)\".format(total_time, minutes))\n",
    "    if minutes > 10:\n",
    "        print(\"‚ö†Ô∏è Warning: Runtime exceeds 10 minutes!\")\n",
    "    elif minutes > 5:\n",
    "        print(\"‚ö†Ô∏è Notice: Runtime exceeds 5 minutes.\")\n",
    "\n",
    "    print(\"\\nüìù Summary of Answers:\")\n",
    "    i=0\n",
    "    for _, answer in all_answers:\n",
    "        i+=1\n",
    "        print(i,\". \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89887587-0155-4dda-8013-6a3ec677a9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Starting chunking and storing process...\n",
      "Input text length: 21216 characters\n",
      "Loading sentence transformer model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n",
      "Initializing in-memory Qdrant client...\n",
      "Qdrant client initialized.\n",
      "Creating or recreating Qdrant collection: 'rag_exercise_pdf_collection_fixed_v1'\n",
      "Collection 'rag_exercise_pdf_collection_fixed_v1' created/recreated successfully.\n",
      "Chunking text using RecursiveCharacterTextSplitter (Size: 256, Overlap: 40)\n",
      "Text split into 106 chunks.\n",
      "Embedding chunks and preparing points for Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stnw22\\AppData\\Local\\Temp\\ipykernel_9312\\3557498546.py:105: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and point preparation took 0.62 seconds.\n",
      "Upserting 106 points into collection 'rag_exercise_pdf_collection_fixed_v1'...\n",
      "Upsert operation successful.\n",
      "Chunking and storing completed in 2.64 seconds.\n",
      "--------------------------------------------------\n",
      "üß† Answering questions...\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 2,400 points if I spend $800 on a hotel with a Travel Rewards+ Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6592, Length: 248). Needs 248 chars.\n",
      "    Added chunk 0. Current prompt length: 389/400\n",
      "  Considering chunk 1 (Score: 0.5651, Length: 241). Needs 242 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (631/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 389 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 4,800 points if I spend ◊í‚Äö√ó3,200 using the Classic Rewards Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5577, Length: 248). Needs 248 chars.\n",
      "    Added chunk 0. Current prompt length: 389/400\n",
      "  Considering chunk 1 (Score: 0.5233, Length: 250). Needs 251 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (640/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 389 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stnw22\\AppData\\Local\\Temp\\ipykernel_9312\\906354012.py:101: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  relevant_chunks: List[ScoredPoint] = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 1,500 points if I book a $500 flight with a Travel Rewards+ Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6209, Length: 223). Needs 223 chars.\n",
      "    Added chunk 0. Current prompt length: 364/400\n",
      "  Considering chunk 1 (Score: 0.5651, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (613/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 364 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive ◊í‚Äö√ó45 cashback if I spend ◊í‚Äö√ó3,000 on general purchases with a Cashback Max Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5423, Length: 152). Needs 152 chars.\n",
      "    Added chunk 0. Current prompt length: 293/400\n",
      "  Considering chunk 1 (Score: 0.4804, Length: 243). Needs 244 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (537/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 293 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 1,000 bonus points if I make 5 bookstore purchases over ◊í‚Äö√ó100 with a Student Start C...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6545, Length: 251). Needs 251 chars.\n",
      "    Added chunk 0. Current prompt length: 392/400\n",
      "  Considering chunk 1 (Score: 0.5604, Length: 207). Needs 208 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (600/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 392 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 3,000 points if I spend $1,000 on flights with a Platinum Voyager Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5901, Length: 214). Needs 214 chars.\n",
      "    Added chunk 0. Current prompt length: 355/400\n",
      "  Considering chunk 1 (Score: 0.5842, Length: 252). Needs 253 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (608/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 355 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 3,500 points if I spend ◊í‚Äö√ó1,500 on groceries and ◊í‚Äö√ó1,000 on the e-shop with a Class...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5935, Length: 247). Needs 247 chars.\n",
      "    Added chunk 0. Current prompt length: 388/400\n",
      "  Considering chunk 1 (Score: 0.5734, Length: 251). Needs 252 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (640/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 388 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I avoid foreign transaction fees if I use the Platinum Voyager Card abroad?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5476, Length: 252). Needs 252 chars.\n",
      "    Added chunk 0. Current prompt length: 393/400\n",
      "  Considering chunk 1 (Score: 0.5324, Length: 192). Needs 193 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (586/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 393 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I earn bonus points if I spend over ◊í‚Äö√ó10,000 in a single month with a Platinum Voyager Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5896, Length: 252). Needs 252 chars.\n",
      "    Added chunk 0. Current prompt length: 393/400\n",
      "  Considering chunk 1 (Score: 0.5407, Length: 214). Needs 215 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (608/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 393 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'If I lose my card and use the app◊í‚Ç¨‚Ñ¢s \"Kill Switch\", can I reactivate it later?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6861, Length: 203). Needs 203 chars.\n",
      "    Added chunk 0. Current prompt length: 344/400\n",
      "  Considering chunk 1 (Score: 0.5209, Length: 249). Needs 250 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (594/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 344 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 5,000 points if I spend $1,000 on dining with a Platinum Voyager Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5498, Length: 214). Needs 214 chars.\n",
      "    Added chunk 0. Current prompt length: 355/400\n",
      "  Considering chunk 1 (Score: 0.5486, Length: 252). Needs 253 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (608/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 355 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive ◊í‚Äö√ó200 cashback if I spend ◊í‚Äö√ó2,000 on electronics using the Classic Rewards Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5346, Length: 152). Needs 152 chars.\n",
      "    Added chunk 0. Current prompt length: 293/400\n",
      "  Considering chunk 1 (Score: 0.4858, Length: 214). Needs 215 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (508/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 293 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 6,000 points if I spend ◊í‚Äö√ó3,000 on the Student Start Card for general items?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5820, Length: 251). Needs 251 chars.\n",
      "    Added chunk 0. Current prompt length: 392/400\n",
      "  Considering chunk 1 (Score: 0.5793, Length: 216). Needs 217 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (609/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 392 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I transfer my loyalty points to a family member?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5838, Length: 252). Needs 252 chars.\n",
      "    Added chunk 0. Current prompt length: 393/400\n",
      "  Considering chunk 1 (Score: 0.5756, Length: 211). Needs 212 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (605/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 393 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I combine points from multiple MySapirBank cards into one account?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5621, Length: 174). Needs 174 chars.\n",
      "    Added chunk 0. Current prompt length: 315/400\n",
      "  Considering chunk 1 (Score: 0.5116, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (564/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 315 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'If I make a large purchase just before my card is downgraded, will I receive premium-tier points bas...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.9012, Length: 214). Needs 214 chars.\n",
      "    Added chunk 0. Current prompt length: 355/400\n",
      "  Considering chunk 1 (Score: 0.5886, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (604/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 355 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I dispute the FX rate applied to a transaction 90 days after the purchase?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5531, Length: 203). Needs 203 chars.\n",
      "    Added chunk 0. Current prompt length: 344/400\n",
      "  Considering chunk 1 (Score: 0.4648, Length: 81). Needs 82 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (426/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 344 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I earn loyalty points on transactions that are reversed later by the merchant and still keep the...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.7143, Length: 182). Needs 182 chars.\n",
      "    Added chunk 0. Current prompt length: 323/400\n",
      "  Considering chunk 1 (Score: 0.6643, Length: 211). Needs 212 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (535/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 323 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I use loyalty points to pay off my statement balance as a regular monthly practice?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.8119, Length: 190). Needs 190 chars.\n",
      "    Added chunk 0. Current prompt length: 331/400\n",
      "  Considering chunk 1 (Score: 0.6303, Length: 251). Needs 252 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (583/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 331 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive ◊í‚Äö√ó200 cashback if I spend ◊í‚Äö√ó4,000 on dining using a Cashback Max Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5424, Length: 243). Needs 243 chars.\n",
      "    Added chunk 0. Current prompt length: 384/400\n",
      "  Considering chunk 1 (Score: 0.5172, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (633/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 384 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I earn points if my transaction is still pending on the MySapirBank statement?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.7921, Length: 176). Needs 176 chars.\n",
      "    Added chunk 0. Current prompt length: 317/400\n",
      "  Considering chunk 1 (Score: 0.6353, Length: 216). Needs 217 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (534/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 317 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive 2,000 points if I spend ◊í‚Äö√ó2,000 on transportation using the Student Start Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5796, Length: 216). Needs 216 chars.\n",
      "    Added chunk 0. Current prompt length: 357/400\n",
      "  Considering chunk 1 (Score: 0.5469, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (606/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 357 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  WARNING: Raw response 'Unknown.' -> Cleaned 'Unknown.' could not be mapped. Defaulting to 'Unknown'.\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive a $10 bonus if I refer a friend to MySapirBank?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.4859, Length: 244). Needs 244 chars.\n",
      "    Added chunk 0. Current prompt length: 385/400\n",
      "  Considering chunk 1 (Score: 0.4024, Length: 246). Needs 247 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (632/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 385 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  WARNING: Raw response 'Unknown.' -> Cleaned 'Unknown.' could not be mapped. Defaulting to 'Unknown'.\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive more points if I spend $800 using the Student Start Card than if I spend $600 using t...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5895, Length: 248). Needs 248 chars.\n",
      "    Added chunk 0. Current prompt length: 389/400\n",
      "  Considering chunk 1 (Score: 0.5185, Length: 246). Needs 247 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (636/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 389 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I earn both cashback and loyalty points if I book a $500 hotel with the Travel Rewards+ Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6321, Length: 152). Needs 152 chars.\n",
      "    Added chunk 0. Current prompt length: 293/400\n",
      "  Considering chunk 1 (Score: 0.5996, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (542/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 293 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  WARNING: Raw response 'Unknown.' -> Cleaned 'Unknown.' could not be mapped. Defaulting to 'Unknown'.\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive higher redemption value for loyalty points earned with a Platinum Voyager Card if I u...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6593, Length: 252). Needs 252 chars.\n",
      "    Added chunk 0. Current prompt length: 393/400\n",
      "  Considering chunk 1 (Score: 0.5614, Length: 250). Needs 251 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (644/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 393 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'If I cancel a purchase after redeeming loyalty points for it, can the points be reinstated under cer...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.7842, Length: 182). Needs 182 chars.\n",
      "    Added chunk 0. Current prompt length: 323/400\n",
      "  Considering chunk 1 (Score: 0.7354, Length: 211). Needs 212 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (535/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 323 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I receive a temporary approval immediately if I apply for a card in a MySapirBank branch and pro...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6366, Length: 210). Needs 210 chars.\n",
      "    Added chunk 0. Current prompt length: 351/400\n",
      "  Considering chunk 1 (Score: 0.6002, Length: 53). Needs 54 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (405/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 351 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'If I pay with the Classic Rewards Card on a partner eCommerce store and my total monthly spending is...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5816, Length: 226). Needs 226 chars.\n",
      "    Added chunk 0. Current prompt length: 367/400\n",
      "  Considering chunk 1 (Score: 0.5557, Length: 251). Needs 252 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (619/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 367 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'If a fraud incident occurs 100 days ago, will I be reimbursed under the Zero Liability policy?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6138, Length: 188). Needs 188 chars.\n",
      "    Added chunk 0. Current prompt length: 329/400\n",
      "  Considering chunk 1 (Score: 0.6047, Length: 187). Needs 188 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (517/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 329 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive more value from using the Travel Rewards+ Card or the Cashback Max Card if I spend $1...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6743, Length: 152). Needs 152 chars.\n",
      "    Added chunk 0. Current prompt length: 293/400\n",
      "  Considering chunk 1 (Score: 0.5584, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (542/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 293 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  WARNING: Raw response 'Unknown.' -> Cleaned 'Unknown.' could not be mapped. Defaulting to 'Unknown'.\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I earn more points from the Classic Rewards Card or the Student Start Card if I spend ◊í‚Äö√ó600 on...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6141, Length: 248). Needs 248 chars.\n",
      "    Added chunk 0. Current prompt length: 389/400\n",
      "  Considering chunk 1 (Score: 0.5491, Length: 252). Needs 253 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (642/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 389 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  WARNING: Raw response 'Unknown.' -> Cleaned 'Unknown.' could not be mapped. Defaulting to 'Unknown'.\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will my loyalty points expire if I hold a Travel Rewards+ Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.7804, Length: 250). Needs 250 chars.\n",
      "    Added chunk 0. Current prompt length: 391/400\n",
      "  Considering chunk 1 (Score: 0.7096, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (640/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 391 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I receive cashback and bonus points if I book a hotel with the Travel Rewards+ Card and use it ...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5895, Length: 152). Needs 152 chars.\n",
      "    Added chunk 0. Current prompt length: 293/400\n",
      "  Considering chunk 1 (Score: 0.5697, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (542/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 293 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I use my loyalty points to pay my credit card bill during a promotional campaign?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.8214, Length: 190). Needs 190 chars.\n",
      "    Added chunk 0. Current prompt length: 331/400\n",
      "  Considering chunk 1 (Score: 0.5923, Length: 211). Needs 212 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (543/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 331 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will a purchase made during an exchange rate update be recalculated later?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6183, Length: 194). Needs 194 chars.\n",
      "    Added chunk 0. Current prompt length: 335/400\n",
      "  Considering chunk 1 (Score: 0.5258, Length: 81). Needs 82 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (417/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 335 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can my loyalty points be forfeited if I default on my credit card payments?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.6861, Length: 211). Needs 211 chars.\n",
      "    Added chunk 0. Current prompt length: 352/400\n",
      "  Considering chunk 1 (Score: 0.6437, Length: 190). Needs 191 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (543/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 352 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I earn more than ◊í‚Äö√ó100 value if I redeem 400 loyalty points for cash using the Student Start C...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5691, Length: 251). Needs 251 chars.\n",
      "    Added chunk 0. Current prompt length: 392/400\n",
      "  Considering chunk 1 (Score: 0.5644, Length: 219). Needs 220 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (612/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 392 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I earn double points on the Classic Rewards Card if I shop at partner stores and exceed ◊í‚Äö√ó3,000...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5297, Length: 250). Needs 250 chars.\n",
      "    Added chunk 0. Current prompt length: 391/400\n",
      "  Considering chunk 1 (Score: 0.5234, Length: 226). Needs 227 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (618/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 391 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'If I earn points in USD and redeem them in ILS, will I receive the same value?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.7754, Length: 223). Needs 223 chars.\n",
      "    Added chunk 0. Current prompt length: 364/400\n",
      "  Considering chunk 1 (Score: 0.5553, Length: 211). Needs 212 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (576/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 364 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will my points be reinstated if I cancel a reward redemption 10 days after redeeming?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.8055, Length: 186). Needs 186 chars.\n",
      "    Added chunk 0. Current prompt length: 327/400\n",
      "  Considering chunk 1 (Score: 0.5724, Length: 182). Needs 183 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (510/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 327 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I reactivate a card that was blocked due to a legal investigation?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5656, Length: 203). Needs 203 chars.\n",
      "    Added chunk 0. Current prompt length: 344/400\n",
      "  Considering chunk 1 (Score: 0.5324, Length: 183). Needs 184 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (528/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 344 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'Yes.' as 'Yes'.\n",
      "  Returning final validated answer: 'Yes'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will the MySapirBank mobile app show real-time balance and points?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5007, Length: 253). Needs 253 chars.\n",
      "    Added chunk 0. Current prompt length: 394/400\n",
      "  Considering chunk 1 (Score: 0.4391, Length: 246). Needs 247 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (641/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 394 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  Returning final validated answer: 'Unknown'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will the Cashback Max Card always give 5% cashback on electronics purchases?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.5866, Length: 243). Needs 243 chars.\n",
      "    Added chunk 0. Current prompt length: 384/400\n",
      "  Considering chunk 1 (Score: 0.5431, Length: 252). Needs 253 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (637/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 384 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Will I earn travel miles if I use the Chase Sapphire Preferred÷≤¬Æ Card to book a hotel?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.4842, Length: 152). Needs 152 chars.\n",
      "    Added chunk 0. Current prompt length: 293/400\n",
      "  Considering chunk 1 (Score: 0.4545, Length: 252). Needs 253 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (546/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 293 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating prompt for question: 'Can I avoid annual fees forever with the American Express÷≤¬Æ Gold Card?...'\n",
      "Finding relevant chunks in fixed collection: rag_exercise_pdf_collection_fixed_v1 using TOP_K=3...\n",
      "  Found 3 candidate chunks.\n",
      "Building context string, respecting limit of 400 chars for instruction + context...\n",
      "  Considering chunk 0 (Score: 0.4818, Length: 187). Needs 187 chars.\n",
      "    Added chunk 0. Current prompt length: 328/400\n",
      "  Considering chunk 1 (Score: 0.4721, Length: 248). Needs 249 chars.\n",
      "    Skipped chunk 1. Adding it would exceed limit (577/400). Stopping context assembly.\n",
      "Final prompt created. Added 1 chunks.\n",
      "Final prompt length (instruction + context): 328 characters.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Executing my_call_llm...\n",
      "  Calling the provided 'call_llm' function...\n",
      "  INFO: Interpreted 'No.' as 'No'.\n",
      "  Returning final validated answer: 'No'\n",
      "--------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è Total Runtime: 7.75 seconds (0.13 minutes)\n",
      "\n",
      "üìù Summary of Answers:\n",
      "1 .  Yes\n",
      "2 .  No\n",
      "3 .  Yes\n",
      "4 .  No\n",
      "5 .  Yes\n",
      "6 .  No\n",
      "7 .  No\n",
      "8 .  Yes\n",
      "9 .  Unknown\n",
      "10 .  Yes\n",
      "11 .  No\n",
      "12 .  No\n",
      "13 .  No\n",
      "14 .  No\n",
      "15 .  Yes\n",
      "16 .  No\n",
      "17 .  No\n",
      "18 .  Yes\n",
      "19 .  No\n",
      "20 .  Unknown\n",
      "21 .  No\n",
      "22 .  Unknown\n",
      "23 .  Unknown\n",
      "24 .  No\n",
      "25 .  Unknown\n",
      "26 .  Yes\n",
      "27 .  No\n",
      "28 .  No\n",
      "29 .  No\n",
      "30 .  No\n",
      "31 .  Unknown\n",
      "32 .  Unknown\n",
      "33 .  No\n",
      "34 .  No\n",
      "35 .  Yes\n",
      "36 .  No\n",
      "37 .  Yes\n",
      "38 .  No\n",
      "39 .  No\n",
      "40 .  No\n",
      "41 .  No\n",
      "42 .  Yes\n",
      "43 .  Unknown\n",
      "44 .  No\n",
      "45 .  No\n",
      "46 .  No\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE\n",
    "run_rag_pipeline(PDF_PATH,QUESTIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662ffdc-cedc-4bf8-9779-31dea948d522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4dcad-03ed-485a-913d-99e8b3d86dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
