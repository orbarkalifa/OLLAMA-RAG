{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5434f62b",
   "metadata": {},
   "source": [
    "# üß™ Exercise 3: PDF Question Answering using Chunking, Vector Search, and LLM\n",
    "\n",
    "In this exercise, you'll complete a **retrieval-augmented generation (RAG)** pipeline that:\n",
    "- Chunks and embeds the content of a PDF\n",
    "- Stores the chunks in an in-memory vector database (Qdrant)\n",
    "- Uses a local LLM to answer yes/no/unknown questions based strictly on the PDF content\n",
    "\n",
    "You will implement the missing components of this pipeline, focusing on document chunking, retrieval, and prompt construction.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "Your objective is to build a system that can answer **yes**, **no**, or **unknown** questions based solely on the information in a given **PDF file**.\n",
    "\n",
    "- The answer must be **one word only**: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`\n",
    "- The LLM must not use external knowledge ‚Äî it must rely **only** on content retrieved from the PDF\n",
    "- The total prompt sent to the LLM is limited to **2000 characters**, including:\n",
    "  - The instruction\n",
    "  - Retrieved context chunks  \n",
    "  *(üö´ The question itself is not included in this limit and will be added separately)*\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What You Need to Do\n",
    "\n",
    "You must implement the following three core functions:\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ `chunk_and_store(text: str) -> tuple[QdrantClient, SentenceTransformer]`\n",
    "\n",
    "This function prepares the document for retrieval.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Chunk the input `text` into smaller segments\n",
    "- Encode each chunk into a vector using a pre-trained embedding model such as `all-MiniLM-L6-v2`\n",
    "- Store the vectors in an in-memory **Qdrant** database using `QdrantClient(\":memory:\")`\n",
    "- Store relevant metadata for each chunk (e.g., start offset, method)\n",
    "\n",
    "**Returns:**\n",
    "- `client`: a Qdrant client object that contains the embedded chunks\n",
    "- `model`: the SentenceTransformer model used for encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ `create_prompt(question: str, client: QdrantClient, model: SentenceTransformer) -> str`\n",
    "\n",
    "This function builds the prompt to be sent to the LLM.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Retrieve the top-k most relevant chunks from Qdrant using the question as a query\n",
    "- Construct a prompt that includes:\n",
    "  - A fixed instruction (you may define this in the function)\n",
    "  - The most relevant retrieved chunks\n",
    "- The full prompt must not exceed **400 characters**, excluding the question\n",
    "\n",
    "**Returns:**\n",
    "- A string containing the prompt (instruction + context), **excluding** the question\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ‚úÖ `my_call_llm(prompt: str, question: str) -> str`\n",
    "\n",
    "This function provides an interface to the LLM, but must not invoke the LLM directly.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Optionally apply logic to enhance or adapt the query (e.g., pre-processing the prompt, logging, enforcing formatting rules)\n",
    "- Call the provided `call_llm(prompt, question)` function to actually interact with the model\n",
    "- Return the result unchanged, or with controlled, explainable adjustments **that do not modify the content of the LLM‚Äôs response**\n",
    "\n",
    "**Rules:**\n",
    "- ‚ùå Must **not** embed, re-embed, or analyze any part of the original document or its chunks\n",
    "- ‚ùå Must **not** call `subprocess`, `ollama`, or any direct LLM API\n",
    "- ‚úÖ Must **only** call `call_llm(prompt, question)` to obtain the response\n",
    "\n",
    "**Returns:**\n",
    "- A string (typically `\"Yes\"`, `\"No\"`, or `\"Unknown\"`) returned by the LLM, possibly post-processed for stability, format, or logging\n",
    "\n",
    "**Purpose:**\n",
    "- This function acts as a controlled gateway to LLM usage, allowing improvements in how prompts are used or tracked, without modifying or reprocessing the document or query logic\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> üí° **Tip:** You are encouraged to define helper functions to simplify your code and improve readability.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Provided Function (DO NOT CHANGE)\n",
    "\n",
    "#### ‚úÖ `call_llm(prompt: str) -> str`\n",
    "\n",
    "This function is already implemented for you.\n",
    "\n",
    "- It calls the `llama3.2:3b` model via `ollama`\n",
    "- Receives the question and your constructed prompt\n",
    "- Returns the LLM's answer (expected: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`)\n",
    "\n",
    "You do **not** need to re-implement or modify this function.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Evaluation Criteria\n",
    "\n",
    "- Your system will be evaluated using a **corpus of 100 questions** on a **known PDF document**\n",
    "- You will be given in advance a **sample of 20 questions** from the evaluation corpus for development and testing\n",
    "- Your code must generate the correct **yes/no/unknown** answers for the full 100-question set\n",
    "- **Total execution time** will be measured for the entire run (reading, chunking, querying, and answering)\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Restrictions\n",
    "\n",
    "- **Do not** modify any code cell marked with `# DO NOT CHANGE`.\n",
    "- **Do not** override any variable or function defined in those protected cells.\n",
    "- Your code must run successfully in the **Lab 10002** environment (`GenAI025_CUDA`), using only the libraries provided.\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Tip:** Write clean, modular code. Aim for accuracy, clarity, and runtime efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Good luck!\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Points Deduction Rules\n",
    "\n",
    "1. **Modifying restricted code**  \n",
    "   - Changing any `# DO NOT CHANGE` cell or variable: **‚Äì50 points**\n",
    "\n",
    "2. **Importing any additional library**  \n",
    "   - Importing any library that is **not already used** in the template: **‚Äì5 points per library**  \n",
    "   - ‚úÖ *No penalty* for importing additional modules or functions from libraries that are already used (e.g., importing more from `langchain` or `sentence_transformers`)\n",
    "\n",
    "3. **Code compatibility**  \n",
    "   - Code fails to run in Lab 10002: **‚Äì100 points**\n",
    "\n",
    "4. **Execution time (total run of 100 questions)**  \n",
    "   - Runs for **5‚Äì10 minutes**: **‚Äì30 points**  \n",
    "   - Runs for **>10 minutes**: **‚Äì100 points**\n",
    "\n",
    "5.  **Violating restrictions inside `my_call_llm()`**  \n",
    "   - ‚ùå Must **not** embed, re-embed, or analyze any part of the original document or its chunks  \n",
    "   - ‚ùå Must **not** call `subprocess`, `ollama`, or any direct LLM API  \n",
    "   - ‚úÖ Must **only** call `call_llm(prompt, question)` to obtain the response  \n",
    "   - Penalty: **‚Äì100 points**\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Final Score Calculation\n",
    "\n",
    "$$\n",
    "\\text{Final Score} = \\min \\left(100,\\ \\frac{\\text{Your correct answers}}{\\text{Gadi‚Äôs correct answers}} \\times 100 \\right) - \\text{Total Deductions}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üìå *Submit clean, working code. Only modify what you're allowed to. You got this!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495b5d6-7594-41b4-a568-1fe3e971acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PATH According to your configuration.\n",
    "PDF_PATH = \"MyBank Credit Card Brochure.pdf\"\n",
    "QUESTIONS_PATH = \"questions.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afada56a-bf9e-4f76-a4fa-ad69f8e43b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "import fitz\n",
    "import os\n",
    "import uuid\n",
    "# import spacy\n",
    "import subprocess\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    NLTKTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, PointStruct\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d21727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "def load_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e838a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "def load_questions(questions_path):\n",
    "    with open(questions_path, \"r\") as f:\n",
    "        questions = [q.strip() for q in f.readlines() if q.strip().endswith('?')]\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f47aeb",
   "metadata": {},
   "source": [
    "### ‚úÖ Task 1: Implement `chunk_and_store(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def chunk_and_store(text: str):\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller chunks and stores them in a vector database or an internal memory structure.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed. This should be a large block of text (e.g., a document, an article, or a report).\n",
    "\n",
    "    Behavior:\n",
    "    --------\n",
    "    1. The function splits the input `text` into manageable chunks based on predefined chunking rules \n",
    "       (e.g., maximum character count, sentence boundaries, semantic meaning).\n",
    "    2. Each chunk is optionally enriched with metadata (e.g., chunk number, character offsets, original document ID).\n",
    "    3. Each chunk is stored in a storage system such as:\n",
    "       - An in-memory list or dictionary (for simple setups)\n",
    "       - A vector database (e.g., Qdrant, FAISS, ChromaDB) after embedding the chunk using an encoder model\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    client : qdrant_client.QdrantClient\n",
    "        A Qdrant client object that contains the embedded and stored chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used for embedding the text chunks.\n",
    "   \n",
    "    Notes:\n",
    "    -----\n",
    "    - If using a vector database, the chunk is first passed through an embedding model to create a vector representation.\n",
    "    - Chunking methods might vary (e.g., fixed-size, sentence-based, semantic-split) depending on implementation details.\n",
    "    - The function assumes that the storage backend is already initialized and ready for storing chunks.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `text` is empty or not a valid string.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> chunk_and_store(\"This is a long article about machine learning...\")\n",
    "    # Splits the article into chunks and stores them internally or externally.\n",
    "\n",
    "    \"\"\"\n",
    "    # Implementation goes here\n",
    "    # TODO: implement chunking using multiple strategies\n",
    "    # TODO: create in-memory Qdrant collection\n",
    "    # TODO: embed each chunk and store in the DB with metadata (chunking method, start_offset)\n",
    "    \n",
    "        # Preprocess PDF text\n",
    "    text = text.replace('-\\n', '').replace('\\n', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    # Initialize model + in-memory Qdrant\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    dim = model.get_sentence_embedding_dimension()\n",
    "    client = QdrantClient(':memory:')\n",
    "    client.recreate_collection(\n",
    "        collection_name='pdf_chunks',\n",
    "        vectors_config=VectorParams(size=dim, distance='Cosine'),\n",
    "    )\n",
    "    # Chunk into ~400-char windows with 200-char overlap\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=r\"\\n\\n|[\\.!\\?]\\s|\\n|\\s\",\n",
    "        is_separator_regex=True,\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    # Embed & store\n",
    "    points = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        vec = model.encode(chunk).tolist()\n",
    "        points.append(PointStruct(id=idx, vector=vec, payload={'text': chunk}))\n",
    "    client.upsert(collection_name='pdf_chunks', points=points)\n",
    "    return client, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169634c",
   "metadata": {},
   "source": [
    "### ‚úÖ Task 2: Implement `create_prompt(question, client, model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question: str, client, model):\n",
    "    \"\"\"\n",
    "    Creates a context-only prompt for an LLM by retrieving relevant chunks from a vector database \n",
    "    based on a user question, using a vector similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    question : str\n",
    "        The input question provided by the user. It should be a natural language query.\n",
    "\n",
    "    client : qdrant_client.QdrantClient\n",
    "        The Qdrant client connected to the database that contains stored and embedded text chunks.\n",
    "\n",
    "    model : sentence_transformers.SentenceTransformer\n",
    "        The SentenceTransformer model used to encode the input question into a vector embedding \n",
    "        for similarity search.\n",
    "\n",
    "    Behavior:\n",
    "    --------\n",
    "    1. The function encodes the input `question` into a vector using the provided `model`.\n",
    "    2. It queries the `client` (Qdrant database) using vector similarity search to find the most relevant chunks.\n",
    "    3. It assembles a prompt by combining the retrieved chunks and other info (but without adding the question itself).\n",
    "    4. The resulting prompt consists **only of context**, intended to be passed separately along with the question \n",
    "       in a later step when calling the LLM.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    prompt : str\n",
    "        A fully formatted prompt string. \n",
    "        **The user's question is NOT included in the returned prompt.**\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The search typically retrieves the top-k most similar chunks (e.g., top 5).\n",
    "    - Retrieved chunks are usually concatenated together, separated by delimiters (e.g., \"\\n\\n\").\n",
    "    - The question should be provided separately to the LLM after sending the prompt, or combined externally later.\n",
    "    - This function assumes that both the client and model are already initialized and ready to use.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input `question` is empty or not a valid string.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> context_prompt = create_prompt(\"What benefits does the Platinum Voyager Card offer?\", client, model)\n",
    "    >>> print(context_prompt)\n",
    "    \"Context:\\n<retrieved chunks>\"\n",
    "\n",
    "    # Later, when calling the LLM:\n",
    "    # final_prompt = context_prompt + \"\\n\\nQuestion:\\nWhat benefits does the Platinum Voyager Card offer?\"\n",
    "    \"\"\"\n",
    "    # TODO: use find_chunks()\n",
    "    # TODO: build the prompt with CONTEXT_HEADER and top chunks\n",
    "    # TODO: truncate to PROMPT_CHAR_LIMIT if needed\n",
    "    # 1) pull out digits from the question\n",
    "    num = \"\".join(ch for ch in question if ch.isdigit())\n",
    "    # 2) embed + retrieve top-20\n",
    "    q_vec = model.encode(question).tolist()\n",
    "    results = client.search(\n",
    "        collection_name='pdf_chunks',\n",
    "        query_vector=q_vec,\n",
    "        limit=20,\n",
    "        with_payload=True,\n",
    "        with_vectors=False,\n",
    "    )\n",
    "    # 3) if there's a number, filter chunks to those containing it\n",
    "    if num:\n",
    "        numeric_hits = [\n",
    "            h for h in results\n",
    "            if num in h.payload[\"text\"].replace(\",\", \"\")\n",
    "        ]\n",
    "        candidates = numeric_hits if numeric_hits else results\n",
    "    else:\n",
    "        candidates = results\n",
    "    # 4) hybrid re-rank by token overlap\n",
    "    tokens = [w.strip(\".,?!;:\").lower() for w in question.split() if len(w) > 2]\n",
    "    scored = []\n",
    "    for h in candidates:\n",
    "        txt = h.payload[\"text\"].lower()\n",
    "        overlap = sum(1 for t in tokens if t in txt)\n",
    "        bonus = (0.1 * overlap / len(tokens)) if tokens else 0\n",
    "        scored.append((h.score + bonus, h))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_hits = [h for _, h in scored[:6]]\n",
    "    # 5) assemble ‚â§400-char prompt\n",
    "    instr = \"Answer Yes/No/Unknown:\"\n",
    "    prompt = instr + \"\\n\\n\"\n",
    "    for h in top_hits:\n",
    "        chunk = h.payload[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        entry = chunk + \"\\n\\n\"\n",
    "        if len(prompt) + len(entry) > 395:\n",
    "            break\n",
    "        prompt += entry\n",
    "    return prompt[:400].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# LLM via Ollama\n",
    "def call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls a local LLM using the Ollama CLI and returns the model's response.\n",
    "\n",
    "    This function sends a prompt to the locally hosted `llama3.2:3b` model via the `ollama` command-line interface.\n",
    "    It ensures the prompt does not exceed 500 characters and captures the model's output.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The full input prompt to be sent to the LLM. It should include context and instructions,\n",
    "                      but not the question itself if using external control.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw response generated by the model. If the model call times out, returns \"Unknown\".\n",
    "\n",
    "    Notes:\n",
    "        - The prompt is truncated to a maximum of 2000 characters before being sent.\n",
    "        - The model is expected to return a one-word answer such as \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    prompt = prompt[:2000] + \"\\nQuestion: \" + question\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3.2:3b\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=30\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd328aed-0af0-4800-bdb4-89c00011f208",
   "metadata": {},
   "source": [
    "### ‚úÖ Task 3: Implement `my_call_llm(prompt: str, question: str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62c34f-e34c-49bf-8036-673318ac2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    A wrapper function for controlled interaction with the local LLM.\n",
    "\n",
    "    This function allows for preprocessing, logging, or evaluation logic\n",
    "    around a call to the provided `call_llm()` function, but it must not\n",
    "    directly interact with the LLM (e.g., via subprocess or embedding logic).\n",
    "\n",
    "    üö´ Restrictions:\n",
    "        - Must NOT embed, re-embed, or analyze any part of the original document or its chunks\n",
    "        - Must NOT call `subprocess`, `ollama`, or any direct LLM APIs\n",
    "        - Must ONLY interact with the LLM via the provided `call_llm(prompt, question)` function\n",
    "\n",
    "    ‚úÖ Allowed:\n",
    "        - Logging or printing\n",
    "        - Handling empty prompts or question formats\n",
    "        - Calling `call_llm()` multiple times for retry logic or consistency checking\n",
    "        - Standard string manipulations (if needed)\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The constructed prompt (instructions + context, excluding the question).\n",
    "        question (str): The original user question (to be passed to the LLM interface).\n",
    "\n",
    "    Returns:\n",
    "        str: - return a one-word answer typically one of \"Yes\", \"No\", or \"Unknown\".\n",
    "\n",
    "    Example:\n",
    "        >>> my_call_llm(\"Context: Data is collected by Google...\", \"Does Google share my location?\")\n",
    "        \"Yes\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Debug passthrough: returns whatever the LLM says\n",
    "    print(\"\\n--- DEBUG PROMPT ---\\n\", prompt)\n",
    "    print(\"--- DEBUG QUESTION ---\\n\", question)\n",
    "    resp = call_llm(prompt, question)\n",
    "    print(\"--- RAW LLM RESPONSE ---\\n\", resp)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a949238-121b-4db6-9e59-b965789d5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "def run_rag_pipeline(pdf_path,questions_path):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for all questions in the input list, printing full results and tracking execution time.\n",
    "\n",
    "    The process includes:\n",
    "    1. Loading and chunking the PDF.\n",
    "    2. Embedding and storing chunks in Qdrant.\n",
    "    3. Answering each question using a locally hosted LLM (via Ollama).\n",
    "    4. Printing the full Q&A pairs.\n",
    "    5. Reporting total runtime with a warning if the run exceeds 5 or 10 minutes.\n",
    "    6. Printing a summary of answers only (one per line).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    text = load_pdf(pdf_path)\n",
    "    questions = load_questions(questions_path)\n",
    "\n",
    "    # Chunk and store once (not inside the loop)\n",
    "    client, model = chunk_and_store(text) # your function\n",
    "\n",
    "    all_answers = []\n",
    "\n",
    "    print(\"üß† Answering questions...\")\n",
    "    for question in questions:\n",
    "        prompt = create_prompt(question, client, model) # your function\n",
    "        answer = my_call_llm(prompt,question)\n",
    "        all_answers.append((question, answer)) \n",
    "        # print(f\"\\nQ: {prompt} \\n Q: {question} \\n A: {answer} \\n {'-'*60} \\n\")\n",
    "        # print(f\"Q: {question} \\n A: {answer} \\n {'-'*60} \\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes = total_time / 60\n",
    "\n",
    "    print(\"\\n‚è±Ô∏è Total Runtime: {:.2f} seconds ({:.2f} minutes)\".format(total_time, minutes))\n",
    "    if minutes > 10:\n",
    "        print(\"‚ö†Ô∏è Warning: Runtime exceeds 10 minutes!\")\n",
    "    elif minutes > 5:\n",
    "        print(\"‚ö†Ô∏è Notice: Runtime exceeds 5 minutes.\")\n",
    "\n",
    "    print(\"\\nüìù Summary of Answers:\")\n",
    "    i=0\n",
    "    for _, answer in all_answers:\n",
    "        i+=1\n",
    "        print(i,\". \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89887587-0155-4dda-8013-6a3ec677a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "run_rag_pipeline(PDF_PATH,QUESTIONS_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
