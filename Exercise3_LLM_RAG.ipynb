{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Exercise 3: PDF Question Answering using Chunking, Vector Search, and LLM\n",
    "\n",
    "In this exercise, you'll complete a **retrieval-augmented generation (RAG)** pipeline that:\n",
    "\n",
    "- Chunks and embeds the content of a PDF\n",
    "\n",
    "- Stores the chunks in an in-memory vector database (Qdrant)\n",
    "\n",
    "- Uses a local LLM to answer yes/no/unknown questions based strictly on the PDF content\n",
    "\n",
    "You will implement the missing components of this pipeline, focusing on document chunking, retrieval, and prompt construction.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "Your objective is to build a system that can answer **yes**, **no**, or **unknown** questions based solely on the information in a given **PDF file**.\n",
    "\n",
    "- The answer must be **one word only**: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`\n",
    "\n",
    "- The LLM must not use external knowledge ‚Äî it must rely **only** on content retrieved from the PDF\n",
    "\n",
    "- The total prompt sent to the LLM is limited to **2000 characters**, including:\n",
    "\n",
    "  - The instruction\n",
    "\n",
    "  - Retrieved context chunks  \n",
    "\n",
    "  *(üö´ The question itself is not included in this limit and will be added separately)*\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What You Need to Do\n",
    "\n",
    "You must implement the following three core functions:\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ `chunk_and_store(text: str) -> tuple[QdrantClient, SentenceTransformer]`\n",
    "\n",
    "This function prepares the document for retrieval.\n",
    "\n",
    "**Responsibilities:**\n",
    "\n",
    "- Chunk the input `text` into smaller segments\n",
    "\n",
    "- Encode each chunk into a vector using a pre-trained embedding model such as `all-MiniLM-L6-v2`\n",
    "\n",
    "- Store the vectors in an in-memory **Qdrant** database using `QdrantClient(\":memory:\")`\n",
    "\n",
    "- Store relevant metadata for each chunk (e.g., start offset, method)\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "- `client`: a Qdrant client object that contains the embedded chunks\n",
    "\n",
    "- `model`: the SentenceTransformer model used for encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ `create_prompt(question: str, client: QdrantClient, model: SentenceTransformer) -> str`\n",
    "\n",
    "This function builds the prompt to be sent to the LLM.\n",
    "\n",
    "**Responsibilities:**\n",
    "\n",
    "- Retrieve the top-k most relevant chunks from Qdrant using the question as a query\n",
    "\n",
    "- Construct a prompt that includes:\n",
    "\n",
    "  - A fixed instruction (you may define this in the function)\n",
    "\n",
    "  - The most relevant retrieved chunks\n",
    "\n",
    "- The full prompt must not exceed **400 characters**, excluding the question\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "- A string containing the prompt (instruction + context), **excluding** the question\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ‚úÖ `my_call_llm(prompt: str, question: str) -> str`\n",
    "\n",
    "This function provides an interface to the LLM, but must not invoke the LLM directly.\n",
    "\n",
    "**Responsibilities:**\n",
    "\n",
    "- Optionally apply logic to enhance or adapt the query (e.g., pre-processing the prompt, logging, enforcing formatting rules)\n",
    "\n",
    "- Call the provided `call_llm(prompt, question)` function to actually interact with the model\n",
    "\n",
    "- Return the result unchanged, or with controlled, explainable adjustments **that do not modify the content of the LLM's response**\n",
    "\n",
    "**Rules:**\n",
    "\n",
    "- ‚ùå Must **not** embed, re-embed, or analyze any part of the original document or its chunks\n",
    "\n",
    "- ‚ùå Must **not** call `subprocess`, `ollama`, or any direct LLM API\n",
    "\n",
    "- ‚úÖ Must **only** call `call_llm(prompt, question)` to obtain the response\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "- A string (typically `\"Yes\"`, `\"No\"`, or `\"Unknown\"`) returned by the LLM, possibly post-processed for stability, format, or logging\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- This function acts as a controlled gateway to LLM usage, allowing improvements in how prompts are used or tracked, without modifying or reprocessing the document or query logic\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> üí° **Tip:** You are encouraged to define helper functions to simplify your code and improve readability.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Provided Function (DO NOT CHANGE)\n",
    "\n",
    "#### ‚úÖ `call_llm(prompt: str) -> str`\n",
    "\n",
    "This function is already implemented for you.\n",
    "\n",
    "- It calls the `llama3.2:3b` model via `ollama`\n",
    "\n",
    "- Receives the question and your constructed prompt\n",
    "\n",
    "- Returns the LLM's answer (expected: `\"Yes\"`, `\"No\"`, or `\"Unknown\"`)\n",
    "\n",
    "You do **not** need to re-implement or modify this function.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Evaluation Criteria\n",
    "\n",
    "- Your system will be evaluated using a **corpus of 100 questions** on a **known PDF document**\n",
    "\n",
    "- You will be given in advance a **sample of 20 questions** from the evaluation corpus for development and testing\n",
    "\n",
    "- Your code must generate the correct **yes/no/unknown** answers for the full 100-question set\n",
    "\n",
    "- **Total execution time** will be measured for the entire run (reading, chunking, querying, and answering)\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Restrictions\n",
    "\n",
    "- **Do not** modify any code cell marked with `# DO NOT CHANGE`.\n",
    "\n",
    "- **Do not** override any variable or function defined in those protected cells.\n",
    "\n",
    "- Your code must run successfully in the **Lab 10002** environment (`GenAI025_CUDA`), using only the libraries provided.\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Tip:** Write clean, modular code. Aim for accuracy, clarity, and runtime efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Good luck!\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Points Deduction Rules\n",
    "\n",
    "1. **Modifying restricted code**  \n",
    "\n",
    "   - Changing any `# DO NOT CHANGE` cell or variable: **‚Äì50 points**\n",
    "\n",
    "2. **Importing any additional library**  \n",
    "\n",
    "   - Importing any library that is **not already used** in the template: **‚Äì5 points per library**  \n",
    "\n",
    "   - ‚úÖ *No penalty* for importing additional modules or functions from libraries that are already used (e.g., importing more from `langchain` or `sentence_transformers`)\n",
    "\n",
    "3. **Code compatibility**  \n",
    "\n",
    "   - Code fails to run in Lab 10002: **‚Äì100 points**\n",
    "\n",
    "4. **Execution time (total run of 100 questions)**  \n",
    "\n",
    "   - Runs for **5‚Äì10 minutes**: **‚Äì30 points**  \n",
    "\n",
    "   - Runs for **>10 minutes**: **‚Äì100 points**\n",
    "\n",
    "5.  **Violating restrictions inside `my_call_llm()`**  \n",
    "\n",
    "   - ‚ùå Must **not** embed, re-embed, or analyze any part of the original document or its chunks  \n",
    "\n",
    "   - ‚ùå Must **not** call `subprocess`, `ollama`, or any direct LLM API  \n",
    "\n",
    "   - ‚úÖ Must **only** call `call_llm(prompt, question)` to obtain the response  \n",
    "\n",
    "   - Penalty: **‚Äì100 points**\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Final Score Calculation\n",
    "\n",
    "$$\n",
    "\\text{Final Score} = \\min \\left(100,\\ \\frac{\\text{Your correct answers}}{\\text{Gadi's correct answers}} \\times 100 \\right) - \\text{Total Deductions}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üìå *Submit clean, working code. Only modify what you're allowed to. You got this!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# SET PATH According to your configuration.\n",
    "\n",
    "PDF_PATH = \"MyBank Credit Card Brochure.pdf\"\n",
    "\n",
    "QUESTIONS_PATH = \"questions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "import fitz\n",
    "\n",
    "import os\n",
    "\n",
    "import uuid\n",
    "\n",
    "# import spacy\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "\n",
    "    CharacterTextSplitter,\n",
    "\n",
    "    NLTKTextSplitter,\n",
    "\n",
    "    SpacyTextSplitter,\n",
    "\n",
    "    RecursiveCharacterTextSplitter\n",
    "\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "from qdrant_client.http.models import VectorParams, PointStruct\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "\n",
    "    return text\n",
    "\n",
    "# DO NOT CHANGE\n",
    "\n",
    "def load_questions(questions_path):\n",
    "\n",
    "    with open(questions_path, \"r\") as f:\n",
    "\n",
    "        questions = [q.strip() for q in f.readlines() if q.strip().endswith('?')]\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set environment variables and constants\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable parallelism for tokenizers to avoid warnings\n",
    "\n",
    "# --- Model and Collection ---\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "COLLECTION_NAME = \"rag_pdf_collection\"\n",
    "\n",
    "# --- Chunking Parameters ---\n",
    "CHUNK_SIZE = 150  # Smaller chunks for more precise retrieval\n",
    "CHUNK_OVERLAP = 50  # Decent overlap to maintain context\n",
    "\n",
    "# --- Retrieval and Prompting Parameters ---\n",
    "TOP_K = 7  # Number of top relevant chunks to retrieve\n",
    "PROMPT_CHAR_LIMIT = 400  # Max characters for the final prompt\n",
    "\n",
    "# --- Helper Function: find_chunks ---\n",
    "def find_chunks(\n",
    "    question: str,\n",
    "    client: QdrantClient,\n",
    "    model: SentenceTransformer,\n",
    "    collection_name: str,\n",
    "    top_k: int = TOP_K\n",
    "):\n",
    "    \"\"\"\n",
    "    Encodes the question and retrieves the top_k most relevant chunks from Qdrant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Embed the question\n",
    "        question_vector = model.encode(question).tolist()\n",
    "\n",
    "        # 2. Query the vector database\n",
    "        search_results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=question_vector,\n",
    "            limit=top_k,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        return search_results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during vector search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### ‚úÖ Task 1: Implement `chunk_and_store(text)`\n",
    "def chunk_and_store(text: str):\n",
    "    \"\"\"\n",
    "    Splits text into chunks, embeds them, and stores them in an in-memory Qdrant vector database.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed (e.g., content of a PDF).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    client : QdrantClient\n",
    "        A Qdrant client connected to the in-memory database containing the embedded chunks.\n",
    "        \n",
    "    model : SentenceTransformer\n",
    "        The SentenceTransformer model used for embedding the text chunks.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(\"Input text cannot be empty and must be a string.\")\n",
    "    \n",
    "    # 1. Initialize the embedding model\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embedding_size = model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    # 2. Initialize in-memory Qdrant client\n",
    "    client = QdrantClient(\":memory:\")\n",
    "    \n",
    "    # 3. Create collection\n",
    "    client.recreate_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=embedding_size, \n",
    "            distance=\"cosine\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 4. Preprocess and chunk the text\n",
    "    # Clean up text by removing hyphenation and normalizing whitespace\n",
    "    text = text.replace('-\\n', '').replace('\\n', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    \n",
    "    # Use RecursiveCharacterTextSplitter for better semantic boundaries\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # 5. Embed and store chunks\n",
    "    points = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        if not chunk_text.strip():\n",
    "            continue\n",
    "            \n",
    "        # Embed the chunk\n",
    "        vector = model.encode(chunk_text).tolist()\n",
    "        \n",
    "        # Create payload with metadata\n",
    "        payload = {\n",
    "            \"text\": chunk_text,\n",
    "            \"chunk_index\": i,\n",
    "            \"char_length\": len(chunk_text)\n",
    "        }\n",
    "        \n",
    "        # Create point with unique ID\n",
    "        point_id = str(uuid.uuid4())\n",
    "        point = PointStruct(id=point_id, vector=vector, payload=payload)\n",
    "        points.append(point)\n",
    "    \n",
    "    # 6. Upsert points to collection\n",
    "    if points:\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "    \n",
    "    return client, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### ‚úÖ Task 2: Implement `create_prompt(question, client, model)`\n",
    "def create_prompt(question: str, client, model):\n",
    "    \"\"\"\n",
    "    Creates a prompt for the LLM by retrieving relevant chunks based on the question.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    question : str\n",
    "        The input question provided by the user.\n",
    "        \n",
    "    client : QdrantClient\n",
    "        The Qdrant client connected to the database containing stored chunks.\n",
    "        \n",
    "    model : SentenceTransformer\n",
    "        The SentenceTransformer model used for encoding the question.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    prompt : str\n",
    "        A formatted prompt string containing instructions and context (‚â§400 chars).\n",
    "    \"\"\"\n",
    "    if not question or not isinstance(question, str):\n",
    "        raise ValueError(\"Question must be a non-empty string.\")\n",
    "    \n",
    "    # 1. Create a concise instruction\n",
    "    instruction = \"Answer with ONLY Yes/No/Unknown based on this context:\"\n",
    "    \n",
    "    # 2. Retrieve relevant chunks\n",
    "    relevant_chunks = find_chunks(\n",
    "        question=question,\n",
    "        client=client,\n",
    "        model=model,\n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    # 3. Extract numbers from the question to prioritize chunks with matching numbers\n",
    "    numbers = ''.join(char for char in question if char.isdigit())\n",
    "    \n",
    "    # 4. Prioritize chunks that contain numbers from the question\n",
    "    if numbers:\n",
    "        # First try chunks with matching numbers\n",
    "        number_chunks = [\n",
    "            hit for hit in relevant_chunks \n",
    "            if any(num in hit.payload[\"text\"] for num in numbers)\n",
    "        ]\n",
    "        \n",
    "        # If we found chunks with matching numbers, prioritize them\n",
    "        if number_chunks:\n",
    "            prioritized_chunks = number_chunks + [\n",
    "                hit for hit in relevant_chunks if hit not in number_chunks\n",
    "            ]\n",
    "        else:\n",
    "            prioritized_chunks = relevant_chunks\n",
    "    else:\n",
    "        prioritized_chunks = relevant_chunks\n",
    "    \n",
    "    # 5. Build context under the character limit\n",
    "    context_parts = []\n",
    "    current_length = len(instruction)\n",
    "    \n",
    "    for hit in prioritized_chunks:\n",
    "        chunk_text = hit.payload[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        \n",
    "        # Check if adding this chunk would exceed the limit\n",
    "        # Add 2 for the space and newline between chunks\n",
    "        if current_length + len(chunk_text) + 2 <= PROMPT_CHAR_LIMIT:\n",
    "            context_parts.append(chunk_text)\n",
    "            current_length += len(chunk_text) + 2\n",
    "        else:\n",
    "            # If we can't fit the whole chunk, try to fit as much as possible\n",
    "            remaining_space = PROMPT_CHAR_LIMIT - current_length - 2\n",
    "            if remaining_space > 30:  # Only add partial chunk if we can fit a meaningful amount\n",
    "                context_parts.append(chunk_text[:remaining_space])\n",
    "            break\n",
    "    \n",
    "    # 6. Assemble the final prompt\n",
    "    context = \" \".join(context_parts)\n",
    "    final_prompt = f\"{instruction} {context}\"\n",
    "    \n",
    "    # 7. Ensure we don't exceed the limit\n",
    "    return final_prompt[:PROMPT_CHAR_LIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# LLM via Ollama\n",
    "def call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls a local LLM using the Ollama CLI and returns the model's response.\n",
    "\n",
    "    This function sends a prompt to the locally hosted `llama3.2:3b` model via the `ollama` command-line interface.\n",
    "    It ensures the prompt does not exceed 500 characters and captures the model's output.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The full input prompt to be sent to the LLM. It should include context and instructions,\n",
    "                      but not the question itself if using external control.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw response generated by the model. If the model call times out, returns \"Unknown\".\n",
    "\n",
    "    Notes:\n",
    "        - The prompt is truncated to a maximum of 2000 characters before being sent.\n",
    "        - The model is expected to return a one-word answer such as \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    prompt = prompt[:2000] + \"\\nQuestion: \" + question\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3.2:3b\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=30\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### ‚úÖ Task 3: Implement `my_call_llm(prompt: str, question: str)`\n",
    "def my_call_llm(prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    A wrapper function for controlled interaction with the local LLM.\n",
    "    \n",
    "    This function calls the provided call_llm function and ensures the response\n",
    "    is one of \"Yes\", \"No\", or \"Unknown\".\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    prompt : str\n",
    "        The constructed prompt (instructions + context, excluding the question).\n",
    "        \n",
    "    question : str\n",
    "        The original user question.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        A one-word answer: \"Yes\", \"No\", or \"Unknown\".\n",
    "    \"\"\"\n",
    "    # Handle empty inputs\n",
    "    if not prompt or not question:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Call the provided LLM function\n",
    "    raw_response = call_llm(prompt, question)\n",
    "    \n",
    "    # Normalize the response\n",
    "    normalized_response = raw_response.strip().lower()\n",
    "    \n",
    "    # Direct matches\n",
    "    if normalized_response == \"yes\":\n",
    "        return \"Yes\"\n",
    "    elif normalized_response == \"no\":\n",
    "        return \"No\"\n",
    "    elif normalized_response == \"unknown\":\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Handle variations and partial matches\n",
    "    if normalized_response.startswith(\"yes\") or \"yes,\" in normalized_response:\n",
    "        return \"Yes\"\n",
    "    elif normalized_response.startswith(\"no\") or \"no,\" in normalized_response:\n",
    "        return \"No\"\n",
    "    \n",
    "    # Look for indicators of uncertainty\n",
    "    uncertainty_phrases = [\n",
    "        \"cannot\", \"can't\", \"don't know\", \"not mentioned\", \"not specified\",\n",
    "        \"unclear\", \"insufficient\", \"not enough\", \"not stated\", \"not provided\",\n",
    "        \"no information\", \"doesn't say\", \"does not say\", \"not clear\"\n",
    "    ]\n",
    "    \n",
    "    if any(phrase in normalized_response for phrase in uncertainty_phrases):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Default to Unknown for any other response\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "def run_rag_pipeline(pdf_path,questions_path):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for all questions in the input list, printing full results and tracking execution time.\n",
    "\n",
    "    The process includes:\n",
    "    1. Loading and chunking the PDF.\n",
    "    2. Embedding and storing chunks in Qdrant.\n",
    "    3. Answering each question using a locally
